#+title: Agenda Notebook
#+author: Marcus Birkenkrahe
#+subtitle: DSC 305 - Machine Learning
#+SEQ_TODO: TODO NEXT IN_PROGRESS | DONE CANCELLED
#+startup: overview hideblocks indent entitiespretty
#+property: :header-args:C:      :main yes :includes <stdio.h> :results output :exports both
#+property: :header-args:python: :session *Python* :python python3 :results output :exports both
#+property: :header-args:R:      :session *R* :results graphics output file :exports both
#+property: :header-args:C++:    :main yes :includes <iostream> :results output :exports both
* DONE Week 1: Overview
#+attr_html: :width 600px:
#+caption: Salvador Dali, The Persistence of Memory (1931)
[[../img/cover.jpg]]

What inspired this painting by Dali? Is the answer a prediction? What
influences the prediction? Can the prediction be validated, and how?

#+begin_quote
- Yes, the answer to the question is a prediction informed by
  historical data.

- It is influenced by Dali's own statements, historical context, such
  as cultural events at the time, and Dali's own experiences.

- The prediction can be partially validated but not proven.

- "The soft watches are an unconscious symbol of the relativity of
  space and time [like in Einstein's theory of special relativity]."
  (Ad√©s)

- "The soft watches were inspired by the surrealist perception of a
  Camembert [cheese] melting in the sun." (Prigogine)
#+end_quote

- [X] Overview: Course content, grading, tools, book, etc.
- [X] On completing DataCamp assignments
- [X] Introduction to Machine Learning (no code)
- [ ] Updating a decision tree model (no code)

** DONE On completing DataCamp assignments

1. Watch the video first with full attention.
2. Write down immediate questions for later.
3. Build a notebook for DataCamp courses.
4. Recreate the individual steps in the video.
5. Create or import datasets.
6. Look up function definitions.
7. Solve the exercises outside of DataCamp.
8. If you loathe Emacs, use DataLab or Colab instead.
9. Don't ignore nagging questions that you have.
10. Start a journal to summarize what you've learnt.

** DONE Review

1. What is "Git"?
   #+begin_quote
   Git is version control software that supports the development and
   maintenance of (large) software projects, e.g. Linux, but also
   any sizeable R package development.
   #+end_quote

2. Which machine learning techniques will we not cover?
   #+begin_quote
   Machine learning techniques that use "black-box" models based on
   neural network architectures.
   #+end_quote

3. What is a "literate program"?
   #+begin_quote
   A program that is written primarily for humans, that includes
   documentation, code, and output, and that can be tangled into
   source code only, or woven into documentation only.
   #+end_quote

4. What is *confirmation bias*?
   #+begin_quote
   Confirmation bias is the tendency to search for, interpret, and
   remember information in a way that *confirms* one's pre-existing
   beliefs or hypothesis (systematic assumptions) while giving less
   consideration to evidence that contradicts them.

   A colloquial term for confirmation bias is "tunnel vision".
   #+end_quote

5. Give an example for confirmation bias?
   #+begin_quote
   Someone who believes that climate change is a hoax might focus on
   cold weather as proof that global warming isn't real while ignoring
   historical evidence to the contrary.

   Conversely, someone who believes in climate change might
   overemphasize a single extreme weather event like a hurricane, as
   proof of climate change while ignoring that individual events
   usually have multiple factors.
   #+end_quote

6. *Let's discuss:* What is a good *response* to confirmation bias?
   #+begin_quote
   1. Actively look for information that challenges your beliefs.
   2. Engage in civilized debate with those who disagree with you.
   3. Ask neutral questions - not "Why" but "What" and "How".
   4. Rely on multiple sources created over a longer period of time.
   5. Challenge your own views through systematic self-reflection.
   6. Balance data-driven, mathematical, and heuristic methods.
   7. Make yourself more aware of the relative meaning of objectivity.
   8. Take time out from your hobby horses.
   9. Avoid social media like the plague.
   10. Eat and sleep well.
   #+end_quote

7. What's a "captcha"?
   #+begin_quote
   "Captcha" stands for "Completely Automated Public Turing" test to
   tell computers and humans apart. They range from media-based (e.g.
   text/image recognition) to behavior-based (e.g. motion patterns).
   #+end_quote

8. What's OCR?
   #+begin_quote
   Many old books and newspapers were digitized using Optical
   Character Recognition (OCR) software. When OCR struggled with
   certain words or symbols, von Ahn's reCaptcha presented users with
   two words: One that the software had difficulty recognizing, and
   one with a known answer. If their response to the known word was
   correct, then their response to the unknown word was used to
   improve the digitization process - an example of both
   "crowdsourcing" (as a human process) and labeling (as a machine
   learning step). Check out von Ahn's own explanation of "human
   computation" ([[https://youtu.be/PQ-xzwj_p_4?si=vD6-OoHcIwMWHnmh][BigThink, 2014]]) - shared in the group chat.
   #+end_quote

* DONE Week 2: Introduction to machine learning

- [ ] Extending and testing a decision tree model
- [ ] *Lecture*: Introduction to machine learning

** DONE Testing and improving a classification model (lab)

- This is an example of a classroom exercise that you can use to get
  bonus points (which you may need to balance bad test results or
  missing assignments).

- If you missed class, or if you did not finish in class, you can
  submit this by the deadline. If you submit it after the deadline, or
  if you submit incomplete solutions, you lose points.


** DONE Lesson Review/Test Preview:

1. Which of these is an optimization task for machine learning?
   #+begin_quote
   1) Improve energy use in an office
   2) Predict election outcomes
   3) Identify email spam
   4) Detect fraudulent transactions
   #+end_quote
   #+begin_quote
   While all options involve ML tasks like classification or
   prediction, optimization refers to improving a process or a
   resource allocation. Prediction and classification can *indirectly*
   help to improve processes but that's not the primary purpose.

   A *task* is something that you do directly. A *goal* is something that
   may be achieved indirectly by performing (many) tasks. A *strategy*
   is a process (usually of multiple steps) to achieve a goal.

   Example: Spam identification
   - Identifying whether an email is spam is a classification *task*.
   - Improving the time spent on processing emails is a *goal*.
   - Using ML to classify spam, refine models over time, and automate
     filtering to unclutter the inbox is a *strategy*.

   Example: Optimize energy use
   - Balance different energy sources with ML is a *task*.
   - Reduce overall energy costs while maintaining comfort is a *goal*.
   - Using ML to predict energy usage patterns, adapt settings in
     real-time, and balance power sources is a *strategy*.
   #+end_quote

2. What is a key limitation of ML models? Example?
   #+begin_quote
   Lack of flexibility. The model needs to be retrained, retuned and
   retested if new data appear, and it can not infer logical steps
   outside of its specific predictive or classification task.

   Retraining especially (depending on the data) can come at a high
   cost. An example are generative AI models (LLMs), or automated
   driving models with real-world edge cases not found in training
   data, or when sensors deliver poor data in real-time.
   #+end_quote

3. What's a major challenge when processing human language, and why?
   #+begin_quote
   The difficulty in interpreting audio and text data accurately.

   Why: Variability of spoken language (dialects, intonations, noise),
   temporal nature of data (time-series), multi-modality (emotion,
   emphasis, intonation), and lack of annotated data.
   #+end_quote

4. Which ethical issues with ML can you think of?
   #+begin_quote
   1) Privacy (training/test data)
   2) Bias (e.g. educational ML)
   3) Reliability in critical situations (e.g. medical ML)
   4) Social pressure (to adopt or not adopt AI)
   5) Copyright issues (e.g. public/private training data)
   6) Decision power (e.g. self-driving cars, military applications)
   7) Addiction and compulsion (to use AI instead of analog/manual)
   8) Cheating (e.g. students)
   9) Accountability (e.g. black-box methods like neural nets)
   10) Job displacement and economic inequality (AI replacing job)
   #+end_quote

5. Which data can and should be anonymized and is that relevant for
   ML?
   #+begin_quote
   All confidential data (e.g. financial, health, survey) should be
   anonymized. ML makes this difficult because it can use existing
   features to identify/reconstruct actors and data that were deleted.
   #+end_quote

6. Define supervised and unsupervised learning with an example each.
   #+begin_quote
   1) *Supervised ML:* Training a model on a labeled dataset - each
      input has a corresponding output. Model learns to map inputs to
      outputs, and can apply this to unknown inputs. *Example:* Medical
      diagnosis (= output) based on observed patient data (= input).

   2) *Unsupervised ML*: Training a model on an unlabeled dataset -
      input is a pattern or a grouping, desired output is a structure
      to the pattern or grouping. *Example*: Segmentation of customers
      into clusters with similar buying behavior (= output) based on
      customer purchasing behavior (= input).
   #+end_quote

7. When and why are datasets split in machine learning? Issues?
   #+begin_quote
   Create separate training data (to build the model) and test data
   (to evaluate the model). Issues include existing patterns (bias) in
   the data (lack of randomization), missing data, missing features.
   #+end_quote

8. What are the earliest historical roots of machine learning?
   #+begin_quote
   Galton's statistical modeling using linear regression - his
   presentation [[https://archive.org/details/1877GaltonTypicalLawsOfHeredity/mode/2up]["Typical Laws of Heredity" (1877)]].
   #+end_quote

9. Is there a limit to the features that can (or should) be used?
   #+begin_quote
   There is no theoretical limit, quite the opposite.
   #+end_quote

10. What should you maximize, and what should you minimize: Sample or
    population?
    #+begin_quote
    You should maxmimize the sample (the data you're using), and you
    cannot manipulate the population because it's a real world fact.
    #+end_quote

* DONE Week 3: Machine learning models overview

- [X] *Projects:* Enter your ideas now - sprint review: Fri 7-Feb
- [X] *Review*: DataCamp - Understanding Machine Learning Models
- [X] *Lecture*: Intro to ML with code along (continued)
- [X] *Test 2* is live - deadline Tuesday 4-Feb (100%).
- [X] The test does not include today's content.
- [X] Next up: R review

** Projects - sprint review coming soon
#+attr_html: :width 600px:
[[../img/projects.png]]

** DataCamp review questions and answers (test 2)

1) What is the difference between *classification* and *regression*?
   Provide examples of each.
   #+begin_quote
   - *Classification* predicts a category, such as "Yes" or "No" (e.g.,
     "Is this email spam?").
   - *Regression* predicts a continuous value, such as a temperature or
     stock price (e.g., "What will the temperature be tomorrow?").
   #+end_quote

2) Which *data structures* are suited to classification and regression
   in R?
   #+begin_quote
   - *Categorical* values are best stored as *factor* vectors with *levels*.
   - *Continuous* values are best stored as *arrays* (like *numeric*
     vectors, matrices)
   - Both of these can be stored as *data frames*.
   #+end_quote

   #+begin_src R :results output
     class(matrix())
   #+end_src

   #+RESULTS:
   : [1] "matrix" "array"

3) What if you want to predict a *label* but the *predictors* have all
   different lengths (e.g., because of missing observations)?
   #+begin_quote
   When predictors have different lengths or missing observations,
   common approaches include:
   1. *imputation* (filling in missing values using statistical methods,
      such as mean, median, or regression),
   2. using *algorithms* that can handle missing data directly (e.g.,
      decision trees: skipping missing values during training, or
      averaging at split during prediction),
   3. *transforming* the data to a fixed size (e.g., by aggregating or
      padding).
   4. *ignoring* if the information in the predictors is less relevant.
   #+end_quote

4) What is the role of a *confusion matrix* in evaluating a model's
   performance?
   #+begin_quote
   To understand the power of a model beyond accuracy score.
   #+end_quote

5) There are 3 red ('fraudulent') and 27 blue ('legitimate') dots in
   the picture. How does the confusion matrix look like?
   #+attr_html: :width 250px:
   [[../img/fraud.png]]

   #+attr_html: :width 400px:
   [[../img/confused.png]]

6) Where would "false positives" be found in the image, and which
   color would they have?
   #+begin_quote
   "False positive" = cases that were predicted to be fraudulent (pink
   area) but were actually legitimate (blue): blue dots at the bottom.
   #+end_quote

7) So is this model: Accurate? Sensitive? Specific?
   #+begin_quote
   - It's accurate: 28/30 points are correctly classified.
   - It's moderately sensitive of the transactions true nature (true
     pos/(true pos + false neg)): focus on left half of the matrix.
   - It's not very specific for identifying fraud. (true neg/(true
     neg + false pos)): focus on right half of the matrix.
   #+end_quote

8) Provide a scenario where either accuracy or sensitivity is
   important, but not both.
   #+begin_quote
   - *Accuracy*: all classes are equally important. Example: handwriting
     recognition system. Misclassification is equally important for all
     digits.
   - *Sensitivity*: identifying true positives is more important than
     minimizing false negatives. Example: medical diagnosis. Healthy
     individuals may be flagged for testing (false positives
   #+end_quote

9) What is *overfitting*, and how can it be identified in a model's
   performance?
   #+begin_quote
   Overfitting occurs when a model performs well on training data but
   poorly on testing data, indicating that it memorized the training
   data instead of generalizing.  This can be identified by testing
   the model on a separate dataset and evaluating its performance.
   #+end_quote

10) What are some common techniques for *dimensionality reduction*, and
    why are they useful in machine learning?
    #+begin_quote
    Common techniques include *removing* irrelevant features, *combining*
    correlated features (e.g., height and weight into BMI), or
    *collapsing* features into a single underlying factor.
    #+end_quote

** Model training, generalization, and evaluation

- *Everybody:* What did you learn in the last couple of sessions?

- Summarize:

  1) Models that use prediction to optimize energy consumption

  2) Difference between supervised and unsupervised models

  3) ML involves splitting data into training & test data

  4) Re-captchas and Duolingo as ML applications

  5) The history stats leading to ML (before 1990)

  6) Different types of supervised learning (regression,
     classifiction).

  7) Using unsupervised learning to identify clusters.

- What (if anything) surprised / amazed / shocked you?

  1) Why ML is not used that much in medicine.

  2) How unsupervised ML detects patterns in unlabeled datasets

  3) How easy all of this is!

** Sample answers

- What did you learn: Limits of machine learning

  1) ML models are...
     #+begin_quote
     ...highly inflexible - strictly parametrized and subject to an
     immutable workflow.
     #+end_quote

  2) The ML workflow always includes:
     #+begin_quote
     1. data set preparation and analysis
     2. data transformation
     3. model training & customization
     4. model testing & evaluation
     #+end_quote

  3) Manual examples you saw included:
     #+begin_quote
     Medical diagnosis (decision tree) and Nile data (workflow)
     #+end_quote

  4) =Nile= data were:
     #+begin_quote
     1. stored in an external file (=write=)
     2. read into memory (=read.table=)
     3. transformed to a time-series (=ts=)
     4. modeled statistically (=summary=)
     5. modeled graphically (=boxplot=, =plot=)
     #+end_quote

  5) What remains:
     #+begin_quote
     Training & Generalization & Evaluation
     #+end_quote

- What surprised/amazed/shocked me:
  #+begin_quote
  1. So many new project ideas (check the Google Chat).
  2. AI changes the game somewhat but not much (MLOps).
  3. Most ML isn't generative AI but traditional stats.
  4. High attendance & classroom spirits despite the cold.
  #+end_quote

* DONE Week 4: Modeling assignment, Deep Learning

** TODO Home assignment: Fitting a linear model to compute g
#+attr_html: :width 400px:
[[../img/newton.jpg]]

- Different ways to solve this problem
- Simplest: compute the trendline and read off the new points
- Provide plots in =ggplot= and/or base R
- Provide notebook in Org-mode or another format

* DONE Week 5: R Review (Basics + Managing data)

- [X] *Test 2:* Counted late submissions bc of network isues
- [X] *Test 3* due Friday (max 100%)
- [X] *DataCamp* assignments: I relaxed the deadlines
- [ ] *R Basics review:* vectors, factors, lists, data frames
- [ ] *R Basics review:* Managing data
- [ ] *R Basics review:* Exploring data

** TODO R Review: Data structures

- The DataCamp lessons & our classroom work require "Intermediate R".

- Download and work through: [[https://tinyurl.com/R-data-structures][tinyurl.com/R-data-structures]] (raw).

- Upload your result then let's review together.

** TODO R Review: Manage and explore

- Download and work through:  [[https://tinyurl.com/R-manage-practice][tinyurl.com/R-manage-practice]]

- Download and work through:  [[https://tinyurl.com/R-explore-practice][tinyurl.com/R-explore-practice]]

* DONE Week 6: R Review: Exploring data

- [X] *Test 4* live until Friday, 28 Feb (Mar 4) for 100% (50%)
- [X] *R Basics:* Explore data (finish)
- [X] *Project* feedback in class - what are your takeaways?
- [ ] *Lecture*: Machine Learning Models

** Review questions

1. When downloading data, what're we looking for?
   #+begin_quote
   1. Data types (~logical~, ~numeric~, ~character~)
   2. What is the numbers of rows and columns (~dim~)
   3. Special characters/encoding
   4. Are integers really integers (and not logical/categorical/double)?
   5. Are characters really strings (and not finite clusters/factors)?
   6. Where do the data come from? (Paper?)
   7. Who collected the data? (Author? Government?)
   8. When were the data collected?
   9. What is the provenance (not the provenience)? Different versions?
   10. Are (all) the data real or are any synthetic (imputed, randomized)?
   #+end_quote

2. What's the interquartile range? How is it computed?
   #+begin_quote
   ~IQR~ is the difference between the 3rd (75%) and the 1st (25%)
   quartile (percentile) - any data point that falls between 25% and
   75% of all the points.
   #+end_quote
   #+begin_src R :session *R* :results output :exports both
     args(IQR)
   #+end_src

   #+RESULTS:
   : function (x, na.rm = FALSE, type = 7)
   : NULL

3. What's the =type= parameter in =IQR=? Why does it exist?
   #+begin_quote
   ~type~: an integer selecting a quantile algorithm.

   There are (ca. 20) different ~quantile~ algorithms because in a
   finite dataset, quantiles often fall between data points. The
   differences are especially prominent for small sample sizes.
   #+end_quote

4. What's an outlier?
   #+begin_quote
   A common definition is "any data point that lies outside of IQR *
   1.5 on either side of the IQR = points below Q1-1.5 * IQR, and
   points above Q3 + 1.5 * IQR.

   *Why 1.5?* It's a *heuristic* balancing robustness and sensitivity in
   stats analysis. It assumes a normal-like distribution but because it
   uses the median (not the mean) it's robust against skewedness. It is
   easy to compute, works well, and is non-parametric.
   #+end_quote

5. What's the difference? What's the output?
   #+begin_example
   1. length(which(data > outlier))
   2. sum(data > outlier)
   #+end_example
   #+begin_quote
   1. computes a ~logical~ index vector and returns its ~length~
   2. computes a ~logical~ vector and returns its ~sum~
   #+end_quote
   #+begin_src R :session *R* :results output :exports both
     which(c(100,200,300,400) > 200)
     length(which(c(1,2,3,4) > 2))
     c(1,2,3,4) > 2
     sum(c(1,2,3,4) > 2)   #  more dangerous bc `sum` not generic
   #+end_src

   #+RESULTS:
   : [1] 3 4
   : [1] 2
   : [1] FALSE FALSE  TgRUE  TRUE
   : [1] 2

   #+begin_src R :results output
     methods(summary)
     methods(sum)
   #+end_src

   #+RESULTS:
   #+begin_example
    [1] summary.aov                    summary.aovlist*
    [3] summary.aspell*                summary.check_packages_in_dir*
    [5] summary.connection             summary.data.frame
    [7] summary.Date                   summary.default
    [9] summary.ecdf*                  summary.factor
   [11] summary.glm                    summary.infl*
   [13] summary.lm                     summary.loess*
   [15] summary.manova                 summary.matrix
   [17] summary.mlm*                   summary.nls*
   [19] summary.packageStatus*         summary.POSIXct
   [21] summary.POSIXlt                summary.ppr*
   [23] summary.prcomp*                summary.princomp*
   [25] summary.proc_time              summary.srcfile
   [27] summary.srcref                 summary.stepfun
   [29] summary.stl*                   summary.table
   [31] summary.tukeysmooth*           summary.warnings
   see '?methods' for accessing help and source code
   no methods found
   #+end_example

6. Which plot type (almost) represents the ~summary~? And what's in the
   ~summary~ that's not in this type of plot?
   #+attr_html: :width 400px:
   [[../img/spread.png]]

   #+begin_src R :session *R* :results output :exports both
     summary(mtcars$mpg) # boxplot does not contain the `mean`
   #+end_src

   #+RESULTS:
   :    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
   :   10.40   15.43   19.20   20.09   22.80   33.90

* DONE Week 7: Machine Learning Models & k-NN algorithm

- [X] *Tests & Assignments*: Removing 1 point per late day. Easier for
  me, kinder to you. All close one week after the deadline though.

- [X] *Project* review and 2nd sprint preview (short)

- [X] *Featured model*: k-nearest-neighbor (knn) models

** Project review - Issues and Tips (almost like DSC 205)

1) What is the central purpose of a project proposal?
   #+begin_quote
   To get buy-in from the project sponsor (that's me). Prove to him
   that this is something worth spending a week's time on. Sell a
   product: You + your work, to solve a problem within the framework
   of the project.
   #+end_quote

2) What's the purpose of a sprint review?
   #+begin_quote
   To present your progress (or lack thereof) to the project owner or
   sponsor, and to the other projects, to highlight issues, enter in a
   conversation with the project owner, and to identify the steps for
   the next sprint. This last step was not asked (I should have -
   let's do it now).
   #+end_quote

3) How much time should you put into the project per week?
   #+begin_quote
   The syllabus does not mention the project separately (too hard to
   do the accounting), but it says: You should spend no less than 25
   minutes per day on this course (outside of class), or no less than
   2.5 hours per week. I spend at least 1 hour per day per class (or >
   4 hours per day). If you have 5-6 classes, you should spend 3-4
   hours per day studying for them.

   Within that schedule, at least 1 hour (10 min per day) should go
   towards your project - in practice more like 1/2 day every 2
   weeks - for a total of no less than 24-32 hours (3-4 days).

   Many proposals look as if you spent too little time on the
   proposals. Reading and working through references alone takes time.
   #+end_quote

4) What are inline references and what are they good for?
   #+begin_quote
   Inline references are abbreviated references that need to be
   written out fully in the reference section. They back up claims
   that made in the text that are non-trivial and not generally known
   to be true.

   Examples:
   - "Agents are not typically taught..."
   - "This approach is common..."
   - "It is pointless to try to..."
   - "Datasets are not publicly available..."

   If you don't need inline references, you make no non-trivial
   claims, which could mean that you're not specific enough or that
   you have not researched the background of your problem.
   #+end_quote

5) What is AMA citation and did you cite consistently?
   #+begin_quote
   An AMA citation refers to the citation style outlined in the
   American Medical Association (AMA) Manual of Style, which is
   commonly used in medicine, healthcare, and biological sciences. The
   AMA citation style follows a numbered system where references are
   listed in the order they appear in the text.

   Examples:
   - [Journal Paper] Smith J, Doe A. Neural networks and phase
     transitions. J Comput Neurosci. 2023;45(2):123-135.
     doi:10.1001/jcn.2023.456
   - [Book] Bishop CM. Pattern Recognition and Machine
     Learning. Springer; 2006.
   - [Website] National Institutes of Health. Artificial intelligence
     in medicine. NIH. Published January 10, 2024. Accessed February
     25, 2025. https://www.nih.gov/ai-medicine
   - [Conf Paper] Johnson K, Lee R. Category theory in neural network
     phase transitions. In: Proceedings of the AI & Math Conference;
     2024; New York, NY. Springer; 2024:210-225.

   Who used AMA? I noticed a lot of inconsistencies, and unncecessary
   URLs etc. Many more references than ever (!) but often in shambles.
   #+end_quote

6) How colloquial should a proposal be? How subjective in tone?
   #+begin_quote
   You need to balance personal, practical and professional tone in
   anything you write to anyone for school or business. How much
   depends totally on the audience and the problem.

   There is only one rule for project documentation:
   Personal/subjective remarks need to have a place of their own, and
   they must not bleed into the professional/practical
   documentation. Otherwise you spoil your professional appearance,
   and you make it very hard for anyone except whom you were thinking
   of to understand what's going on.
   #+end_quote

7) Do you need references? Which ones, and why?
   #+begin_quote
   The only situation where you don't need any references is when you
   are working on something that nobody has ever worked on so that
   your choice of topic and method are completely original.

   So you do need them. As many scientific ones (peer-reviewed) as
   possible. You must make an assessment of the validity of your
   sources, and you need to be able to answer for them.
   #+end_quote

8) Do you have to improve your proposal based on the feedback?
   #+begin_quote
   Yes, you should, and you should touch base with me if anything in
   the feedback is unclear. The feedback should be more formal than it
   is - basically I'm only checking if you fulfilled the requirements
   to the letter (e.g. completeness, consistent references etc.)
   and/or in spirit, if it's doable, and if you've done everything you
   could at this point.

   A formal feedback would require a formal list of weighted criteria
   (a rubric). This is what you'll be facing in business not
   academia. Academia is terribly weak this way and much less
   resilient - you can see this in the science funding drama that is
   currently unfolding around us: scientists often don't bother
   justifying their ends, especially if they feel justified by their
   means.

   I don't bother with a formal framework mainly because otherwise I
   would have to spend a lot more time on the projects (like a manager
   would whose main job is managing projects, pennies, and people). So
   this is your show and responsibility much more than it would be in
   the real world.

   Do you have to share your improved proposal with me, unasked?
   Absolutely, if you've got any doubts if you did it right, or if you
   just want to show off a good piece of work (that's never wrong -
   you may want a letter of reference from me one day). This shows
   both diligence and resilience.
   #+end_quote

9) Should you look at the proposals of others?
   #+begin_quote
   Absolutely. Cristian's proposals are the most polished ones. In
   DSC 305, Matthew's proposal has the most meat (data sets and code
   examples attached). Donna's proposal reflects the greatest range
   of ambitions. Gavin's and Isaac's proposals are pithy and to the
   point. Fowler's proposals are fairly personal.

   You should make up your own mind about these. They're all
   interesting, and they all have slight issues here and there.

   You might even give the authors tips based on your experience or
   your views.
   #+end_quote

10) Do you need an AI disclaimer?
    #+begin_quote
    It would be good! Matthew designed one that I asked the AI to
    rewrite and I think it worked quite well (Ariel also had something
    like this in his proposal):

    "In researching for this project, I may have been exposed to
    AI-generated content in search results and news feeds. However, I
    affirm that this work is my own and was completed with academic
    integrity, without relying on AI to circumvent my responsibilities
    as a student."

    If you don't have a disclaimer because you used AI directly, you
    needed to (and still need to) document as much as possible
    (including perhaps by sharing links to your conversation) why and
    how you used the AI.
    #+end_quote

** Litmaps and NotebookLM

1) Create a Litmap for your ML project right now and search for a
   relevant paper.

2) Create a NotebookLM for your ML project right now and upload your
   proposal.

** DataCamp review: kNN classification

1. What is the primary goal of classification in supervised learning?
   #+begin_quote
   Assigning labels to data based on learned patterns.
   #+end_quote

2. In the k-Nearest Neighbors (kNN) algorithm, how is similarity
   between data points measured?
   #+begin_quote
   Using distance metrics like Euclidean distance. There are many
   other measures of similarity & the math theory is HARD.
   #+end_quote

3. Which R library is commonly used to apply the kNN algorithm?
   #+begin_quote
   =class= - there are many others, e.g. =caret= (classification and
   regression training), =kkNN= (Kernel kNN), =FNN= (Fast Nearest
   Neighbor), =mlr3= (Machine Learning in R), and =tidymodels=.

   | Library    | Function            | Features                                  |
   |------------+---------------------+-------------------------------------------|
   | *class*      | ~knn()~               | Simple kNN, no hyperparameter tuning      |
   | *caret*      | ~train(method="knn")~ | Easy tuning, part of unified ML framework |
   | *kknn*       | ~kknn()~              | Weighted kNN, flexible distance functions |
   | *FNN*        | ~knn()~               | Fastest nearest neighbor search           |
   | *mlr3*       | ~lrn("classif.kknn")~ | Modular OOP machine learning framework    |
   | *tidymodels* | ~nearest_neighbor()~  | Tidyverse-compatible machine learning     |

   #+end_quote

   #+begin_src R :session *R*  :results output :exports both
     ## Load the class library
     library(class)

     ## Example dataset: Iris dataset
     data(iris)

     ## Prepare training and testing sets
     classify <- function(x) {
       set.seed(123)  ## For reproducibility
       index <- sample(1:nrow(x), 0.7 * nrow(x))  ## 70% training data
       train_data <- x[index, 1:4]  ## Select feature columns
       test_data <- x[-index, 1:4]  ## Select feature columns for testing
       train_labels <- x[index, 5]  ## Species labels
       test_labels <- x[-index, 5]  ## True labels for testing
       ## Apply kNN with k = 3
       pred <- knn(train = train_data, test = test_data, cl = train_labels, k = 3)
       ## Check accuracy
       accuracy <- sum(pred == test_labels) / length(test_labels)
       ## return accuracy
       return (accuracy)
     }
     accuracy_result <- classify(iris)
     print(paste("Accuracy:", round(accuracy_result * 100, 2), "%"))
   #+end_src

   #+RESULTS:
   : [1] "Accuracy: 97.78 %"

#+begin_src R :session *R* :results output
  str(iris)
#+end_src

#+RESULTS:
: 'data.frame': 150 obs. of  5 variables:
:  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...
:  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...
:  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...
:  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...
:  $ Species     : Factor w/ 3 levels "setosa","versicolor",..: 1 1 1 1 1 1 1 1 1 1 ...

#+begin_src R :session *R* :results output graphics file :file iris.png
  plot(iris)
#+end_src

#+RESULTS:
[[file:iris.png]]

4. How does increasing the value of 'k' in kNN affect the model?
   #+begin_quote
   It smooths the decision boundary but may reduce sensitivity to
   small patterns.
   #+end_quote

5. Why is data normalization important in kNN?
   #+begin_quote
   It ensures that all features contribute equally to distance
   calculations.
   #+end_quote

6. Which function is used to normalize data in R for kNN?
   #+begin_src R :results output
     ## Define a min-max normalization function
     normalize <- function(x) {
       return((x - min(x)) / (max(x) - min(x)))
     }

     ## Example dataset: Creating a sample vector
     sample_data <- c(10, 20, 30, 40, 50, 60, 70, 80, 90, 100)

     ## Applying normalization
     normalized_data <- normalize(sample_data)

     ## Print the original and normalized data
     print("Original Data:")
     print(sample_data)
     print("Normalized Data:")
     print(round(normalized_data*100,2))
   #+end_src

   #+RESULTS:
   : [1] "Original Data:"
   :  [1]  10  20  30  40  50  60  70  80  90 100
   : [1] "Normalized Data:"
   :  [1]   0.00  11.11  22.22  33.33  44.44  55.56  66.67  77.78  88.89 100.00

7. What type of data does kNN assume?
   #+begin_quote
   Numeric data.
   #+end_quote

8. What is the primary drawback of using a very small 'k' in kNN?
   #+begin_quote
   The model becomes sensitive to noise and outliers.
   #+end_quote

9. What is the purpose of training and testing data in kNN?
   #+begin_quote
   To train the model on known data and evaluate its accuracy on
   unseen data.
   #+end_quote

10. What happens when kNN is applied without normalizing features that
    have different scales?
    #+begin_quote
    Features with larger ranges dominate distance calculations (not
    for all measures).
    #+end_quote

* DONE Week 8: kNN & Literature Review
** Literature Review: Fake Papers and the Sokal Hoax

- Post with specific hints on uncovering fake papers in the chat
  ([[https://blog.litmaps.com/p/have-you-cited-a-fake-paper-heres][Litmap]])

- Apart from the obvious (fake citations), it's hard for you to do.

- "Fake" does not just mean "made up": all research is made up to some
  extent - when evidence is collected, a methodology is used, which
  may be so flawed that the research findings are completely wrong.

- "Fake" may also mean "in bad faith" (cutting corners, or
  intellectually dishonest). This is an issue in the humanities.

- The first person to cover this phenomenon with great effect was Alan
  Sokal, a physics professor from NYU ([[https://physics.nyu.edu/sokal/weinberg.html][Weinberg, 1996]]) with
  "Transgressing the boundaries - Toward a transformative hermeneutics
  of quantum gravity" ([[https://physics.nyu.edu/sokal/transgress_v2/transgress_v2_singlefile.html][1995]]).

- [[https://www.theatlantic.com/ideas/archive/2018/10/new-sokal-hoax/572212/][In a more recent example (2018)]], three researchers faked 20 papers
  in fashionable disciplines ("gender studies", "queer studies", "fat
  studies"), of which only 30% were rejected.

- What do we learn from this?
  #+begin_quote
  1. Fake papers go beyond fabrication.
  2. Ideological alignment must not go over scientific validity.
  3. There is a tension between academic freedom and accountability.
  #+end_quote

- What can you do about it with regard to your research project?
  #+begin_quote
  1. Keep the topic - your research question - small and specific.
  2. Limit your sources to well validated sources (many citations,
     clear writing).
  3. Know your few sources well (research them!)
  4. Value primary data (that you take) above all else, and pay
     special attention to the origin of your secondary data.
  5. Consider alternate methodologies and have good reasons why you
     pick one way of doing things (not just "because I could").
  6. Don't be too sure, don't publish prematurely, and show your work
     to many (trustworthy) people for feedback.
  #+end_quote

** Lecture + codealong + practice: kNN classifier algorithms

1) [X] Lecture
2) [ ] Practice (demo: cancer classification with k-NN)
3) [ ] Exercises (classroom or home)
4) [ ] Test 5 - deadline March 21

** Review - kNN

1. What is the main characteristic of k-Nearest Neighbors (k-NN) as a
   classifier that you should remember?
   #+begin_quote
   k-NN does not build a model but classifies based on similarity to
   training examples.
   #+end_quote

2. Why is k-NN referred to as a "lazy learner"?
   #+begin_quote
   k-NN does not generalize from training data but stores it verbatim
   and makes predictions at query time.
   #+end_quote

3. What is the role of the parameter *k* in k-NN?
   #+begin_quote
   The parameter *k* determines the number of nearest neighbors
   considered for classification.
   #+end_quote

4. Why is k-NN susceptible to the "curse of dimensionality"?
   #+begin_quote
   As the number of features increases, distance calculations become
   less meaningful, making classification less effective, because all
   pairs of points become roughly similar (the range of distances gets
   compressed relative to the overall scale of the space). To cover
   the same neighborhood in a high-dimensional space you need
   exponentially more data points.

   Remember: spread & uniformity & sparsity. This is going to become
   an assignment, comparing kNN for 2-dim vs. 20-dim with/out R.
   #+end_quote

5. What type of data normalization is commonly used before applying k-NN?
   #+begin_quote
   Min-max normalization or z-score standardization (involving the
   standard deviation - point density) is used to ensure features have
   comparable scales.
   #+end_quote

6. How does k-NN handle categorical data?
   #+begin_quote
   Categorical data is converted to numerical form using techniques
   like dummy coding, e.g. assigning 0 and 1 to binary categories.
   #+end_quote

7. What is a major weakness of k-NN regarding computational efficiency?
   #+begin_quote
   k-NN requires storing and comparing all training examples, making
   classification slow, especially with large datasets.
   #+end_quote

8. What is the primary similarity measure used in k-NN?
   #+begin_quote
   The Euclidean distance is commonly used, though other metrics like
   Manhattan or cosine similarity may also be applied. In R, check
   =help(dist)=. In Python, see doc for ~Scikit-learn~, ~SciPy~ etc.
   #+end_quote

9. How does k-NN determine the class of a test instance?
   #+begin_quote
   It assigns the class that appears most frequently among the *k*
   nearest neighbors of the test instance.
   #+end_quote

10. What happens if *k* is too large in k-NN?
    #+begin_quote
    A large *k* may cause overgeneralization, making the classifier
    biased towards the majority class and reducing sensitivity to local
    patterns (example of "bias-variance tradeoff").
    #+end_quote

* DONE Week 9: kNN case study - breast cancer diagnosis
#+attr_html: :width 700px :float nil:
#+caption: Paul Davies, North Yorkshire Moors
[[../img/moors.jpg]]

** Agenda

- [X] *Tests*: Former rule (50% after the deadline) reinstalled (because
  there aren't enough days left in the term for the other rule :-) But
  you can complete the tests now at any time for 50% of the points
  until May 2nd (term ends May 9th).

- [X] *Test 5* is live: 50 questions on projects & fake publications &
  knn. Uses lectures and review sessions (available in GitHub) as
  usual. Deadline March 21 (2 weeks).

- [X] *DataCamp assignments:* Extended deadlines for "Logistic
  Regression" (March 21) and "Classification Trees" (April 4) -
  because of spring break (March 22-March 30).

- [X] *kNN bonus exercises:* Two bonus exercises explore normalization
  and choice of k-value for kNN algorithms in the context of the
  breast cancer data. You get my sample solutions upon submission.

- [X] *New bonus kNN exercises*: I posted two new (bonus) exercises for
  kNN both about the "curse of dimensionality", a major issue with kNN
  in higher-dimensional feature spaces. Sample solutions available.

- [X] *Curse of dimensionality*: New PDF posted in chat - read before
  attempting the bonus exercises.

- [X] *Outlook:*
  1) Naive Bayes (w9-w10)
  2) Regression analysis (w11-w12)
  3) k-means clustering (w13-w14)
  4) Projects (w15-w16)

- [X] *kNN case study*: code along! This time, your brain won't hurt
  because it's hard but because we keep going at it again and again...

- *Lesson for life:* You learn something only when you make yourself
  solve a problem even though your brain hurts (BSc). You build a
  skill only by going over the same terrain again and again (different
  pain) until you know it like the back of your hand (PhD).

** Review

1) Come up with one question about kNN.
2) Ask your neighbor.
3) Discuss the answer.

-----
Questions:

** Finish the case study

- To run all of your code blocks: =M-x org-babel-execute-buffer=

- Check that your last output looks like this:
  #+begin_example
    #+RESULTS:
  : [1] 0.00 0.25 0.50 0.75 1.00
  : [1] 0.00 0.25 0.50 0.75 1.00
  #+end_example

- Code along!


* DONE Week 10: Naive Bayes
#+attr_html: :width 400px :float nil:
#+caption: Salvador Dali, The Elephants (1948)
[[../img/dali.jpg]]

- [X] DataCamp review: Naive Bayes
- [X] Lecture: Naive Bayes
- [ ] Case study: Spam filter

** DataCamp review: Naive Bayes

1. *What is the core idea behind Bayesian methods in classification?*
   #+begin_quote
   Using probabilities to predict outcomes based on prior knowledge.
   #+end_quote

2. *How is the probability of an event denoted in probability notation?*
   #+begin_quote
   P(A) - purely conventional but broadly used
   #+end_quote

3. *What is the formula for calculating joint probability of two events?*
   #+begin_quote
   P(A AND B) or ~P (A \cap B)~
   #+end_quote

4. *How is conditional probability defined mathematically?*
   #+begin_quote
   "P(A|B) = P(A AND B) / P(B)" or ~P(A|B) = P(A \cap B) / P(B)~
   #+end_quote

5. *What R function is commonly used to make predictions with a trained
   model? What kind of function is it?*
   #+begin_quote
   ~predict~
   #+end_quote
   #+begin_src R :session *R* :results output :exports both
     args(predict)
     methods(predict)
   #+end_src

   #+RESULTS:
   : function (object, ...)
   : NULL
   : [1] predict.ar*                predict.Arima*             predict.arima0*            predict.glm
   :  [5] predict.HoltWinters*       predict.lm                 predict.loess*             predict.mlm*
   :  [9] predict.nls*               predict.poly*              predict.ppr*               predict.prcomp*
   : [13] predict.princomp*          predict.smooth.spline*     predict.smooth.spline.fit* predict.StructTS*
   : see '?methods' for accessing help and source code

6. *What assumption does Naive Bayes make about predictors?*
   #+begin_quote
   Predictors are independent of each other.
   #+end_quote

7. *What issue arises when an event has a zero probability, and how is
   it typically addressed?*
   #+begin_quote
   Zero probabilities can skew results; the Laplace correction adds a
   small value to avoid this.
   #+end_quote

8. *How can numeric data be prepared for use in Naive Bayes?*
   #+begin_quote
   By binning it into discrete categories.
   #+end_quote

9. *What is the "bag of words" approach used for in classification?*
   #+begin_quote
   Converting text data into a format suitable for modeling.
   #+end_quote

10. *Why is Naive Bayes considered a "naive" approach?*
    #+begin_quote
    It simplifies complex relationships by assuming independence.
    #+end_quote

** Lecture: Naive Bayes + code along case

*Objectives:*
- Classification using Naive Bayes
- Bayes' theorem and naive assumptions
- Text classification use case
- R packages for text mining & visualization
- Application: SMS junk message filter

*Codealong file*: tinyurl.com/naive-bayes-org

** Review: Naive Bayes (lecture)

1. *What is the main purpose of using probabilistic methods in
   classification?*
   #+begin_quote
   To estimate the likelihood (chance) of an outcome based on past
   (prior) data and uncertainty (not knowing the future).
   #+end_quote

2. *How does Bayes‚Äô theorem help in building a classifier?*
   #+begin_quote
   It updates the probability of a class (e.g. =spam=) based on new
   evidence from features (e.g. =Viagra=) or predictors.
   #+end_quote

3. *Why are joint probabilities of independent events multiplied
   instead of added?*
   #+begin_quote
   Because independence means the occurrence of one event doesn‚Äôt
   affect the other, so their combined chance is the product of
   individual probabilities.

   Example: Suppose the chance of rain in the morning is 30% and the
   chance of wind in the afternoon is 40%, and these are independent
   (assuming morning rain doesn‚Äôt affect afternoon wind). The joint
   probability of both is =P(rain and wind) = P(rain) * P(wind) = 0.3 *
   0.4 = 0.12= or 12%.
   #+end_quote

4. *Why are probabilities of mutually exclusive events added instead of
   multiplied?*
   #+begin_quote
   Because mutually exclusive events cannot occur together, so their
   combined chance (one or the other) is the sum of their individual
   probabilities.

   Example: Suppose the chance of rain in the morning is 30% and the
   chance of snow in the morning is 20%, and these are mutually
   exclusive (assuming it can‚Äôt rain and snow at the same time). The
   probability of either is =P(rain or snow) = P(rain) + P(snow) =
   0.3 + 0.2 = 0.5= or 50%."
   #+end_quote

5. *What does "mutually exclusive events" mean for rain and snow in the
   morning?*
   #+begin_quote
   Mutually exclusive events cannot happen at the same time in the
   same place during the specified time period - here the morning.

   If prior/historical data show that out of 100 mornings, 30 had rain
   and 20 had snow (with no mornings having both), the probability of
   either is 50% - the event outcomes don't overlap in a single
   observation.
   #+end_quote

6. *What's "joint probability" of two events "morning rain" and
   "afternoon wind"?*
   #+begin_quote
   The example assumes independence across time periods - it's the
   probability of a single day having both morning rain and afternoon
   wind, based on past days with those combined conditions.

   It's a joint event for that day's record.
   #+end_quote

7. *How is "joint probability" (chance of two events A, B occurring
   together) expressed abstractly? What if they're independent?*
   #+begin_quote
   =P(A \cap B)=. If  independent, =P(A) * P(B)=.
   #+end_quote

8. *How is "mutual exclusiveness" of two events A, B (events cannot*
   *occur together) expressed abstractly? What's the chance of either
   to occur?*
   #+begin_quote
   =P(A \cap B) = 0=, and =P(A \cup B) = P(A) + P(B)=
   #+end_quote

9. *What does the 'naive' assumption in Naive Bayes imply about
   features like =Viagra= for =spam= identification?*
   #+begin_quote
   It assumes all features are independent and equally important in
   predicting the class. This is clearly not usually true.
   #+end_quote

10. If Naive Bayes is so "naive", why does it still work?
    #+begin_quote
    Nobody really knows. But examples may help to illustrate it: In
    text classification (like =spam= identification), words may be
    correlated (like =free= and =cash=) but if their combined presence
    points strongly to one class (like =spam=), the correlation
    reinforces the classification rather than derail it.

    However: See the "frequentist" objection against Bayes below.
    #+end_quote

11. *How does the "law of large numbers" relate to improving
    probability estimates?*
    #+begin_quote
    It suggests that with more trials, the average outcome approaches
    the "true" probability (that would be found based on all available
    prior events).
    #+end_quote

12. *What is the difference between prior and posterior probabilities
    in Bayesian methods?*
    #+begin_quote
    Prior is the initial probability before evidence (can be
    computed); posterior is the updated probability after considering
    evidence (can be predicted)."
    #+end_quote

13. *What do we want to compute using Bayes' formula?*
    #+attr_html: :width 600px
    [[../img/5_bayes_spam_ham.png]]
    #+begin_quote
    Given known (labeled) datasets with known frequencies of =spam=
    vs. =ham= and =spam= / =ham= with or without the term =Viagra=, we want to
    build a model that classifies new messages as =spam= or =ham= depending
    on the presence or absence of the term =Viagra= as our only predictor
    (for now).
    #+end_quote

15. Why is =P(Spam|Viagra)= not equal to =P(Viagra|Spam)=? And how does it
    relate to "time" (not philosophically, though perhaps that, too)?
    #+begin_quote
    1) They measure different things: =P(Spam|Viagra)=, the *posterior*, is
       the probability a message is classified as =spam= given it contains
       the term =Viagra=, while =P(Viagra|Spam)=, the *likelihood*, is the
       probability that the term =Viagra= appears in a =spam= message.

    2) Bayes' theorem can be flipped algebraically (formally) to
       express P(V|S)=P(S|V)*P(V)/P(S).  But =P(Viagra|Spam)= is
       computed directly from data as a conditional frequency within
       =spam=, and not via =P(V)=.

    3) Put differently yet again: P(S|V) normalizes by P(V) - it
       conditions on V being present ("What's the chance of V given
       S"), while P(V|S) normalizes by P(S) - it conditions on S being
       present ("What's the chance of S given V"). When P(S)=P(V), the
       order of conditioning does not matter algebraically but it
       still matters in time.

    4) Identify of these would mean that =spam= messages are exactly as
       common as =Viagra= appearances in =spam=: e.g. if 100 emails contain
       20 spam messages, 20 emails contain the word "Viagra", and 10
       emails are both spam and contain "Viagra":
       - P(Spam) = 20/100 = 0.2
       - P(Viagra) = 20/100 = 0.2
       - P(Viagra|Spam) = 10/20 = 0.5
       - P(Spam|Viagra) = 10/20 = 0.5
    #+end_quote

    #+begin_quote
    Henri Bergson (1859-1941) viewed time as "duration" (dur√©e), a
    continuous flow of experience distinct from the spatialized time
    of science. He argued that clock time misrepresents reality -
    rather, past, present and future blend in consciousness, not a
    series of static points. The scientific world-view imposes an
    artificial lattice or grid imposed on the flux of life.

    In Bergson's view, probability reduces time to a tool for
    counting. It aligns a bit more with Bayesian than with Frequentist
    terms. But both frame time quantitatively. Bergson would say that
    probability captures patterns within a simplified version of time,
    not its lived reality, and it therefore can not represent truth.

    Bergson's books are beautifully written (as a young man, I was a
    great fan of his): He received the Nobel Prize for Literature
    (1927/28).
    #+end_quote

16. *Bonus:* What is Frequentist vs. Bayesian Thinking? (Extension)
    #+begin_quote
    - *Frequentist* thinking views probability as the long-run frequency
      of events, using fixed parameters (e.g., a coin‚Äôs heads
      probability is 0.5) and relying on sample data *without prior
      beliefs*.

    - *Bayesian* thinking treats *probability as a measure of belief*,
      updating prior probabilities (e.g., initial guess of 0.6 for
      heads) with new data via Bayes‚Äô theorem to get a posterior,
      allowing subjective input.

    - *Example:* You flip a coin 100 times and get 52 heads. The
      Frequentist estimate is P(heads) = 52/100 = 0.52, based purely on
      the data, with no prior assumptions. The Bayesian view starts
      with a prior belief that P(heads) = 0.6 for a coin. After
      flipping it 100 times and getting 52 heads, Bayes‚Äô theorem
      updates your belief to a posterior, say P(heads) ‚âà 0.53, blending
      your prior with the data.

    See also: [[https://medium.com/data-science/statistics-are-you-bayesian-or-frequentist-4943f953f21b][Bayesian vs. Frequentist by Cassie Kozyrkov (Google)]].
    #+end_quote

** TODO Case study

- To code along with the lecture, download the practice file from
  GitHub, complete the file and upload it to Canvas:
  [[https://tinyurl.com/naive-bayes-case][tinyurl.com/naive-bayes-case]]
  
* DONE Week 11: Naive Bayes case study with text mining
#+attr_html: :width 300px :float nil:
[[../img/naive1.png]] 

** 2nd sprint review feedback

- [[https://drive.google.com/file/d/15vyt1mPuFFcBGosxAv1oxM3SwqUipFqS/view?usp=drive_link][Discuss Matthew's literature review]]. Usually, a review is focused on
  one research question, or a group of strongly correlated
  questions. Its only purpose is to lay out the available literature,
  in order to estimate the state of the art with regard to pursuits of
  that research question. A literature review section in a paper does
  the same thing. This paper is a valid attempt, too, but a different
  beast altogether. Its main issue is the missing methodology.

- [[https://docs.google.com/document/d/1u1AFSUufeJvga0rQ7XqkSsYPChKuNfKf/edit?usp=sharing&ouid=102963037093118135110&rtpof=true&sd=true][Cristian's literature review is also very good]]. It's more
  traditional. Inline references should not be numbers, however (so
  that one does not have to continously go back and forth to the
  reference section). The review ties nicely into his project.


** Case study
#+attr_html: :width 300px :float nil: 
[[../img/naive3.png]] 

- To code along with the lecture, download the practice file from
  GitHub, complete the file and upload it to Canvas:
  [[https://tinyurl.com/naive-bayes-case][tinyurl.com/naive-bayes-case]]

** Text mining pipeline challenge

Do what we did in class as a pipeline. Details in Canvas.


* Week 12: Finish Naive Bayes Case and ... Regression (perhaps)
#+attr_html: :width 600px :float nil:
[[../img/regression.png]]

- [ ] *New test*: Naive Bayes (by Friday, April 11) - 4 more
- [ ] *DataCamp lesson*: Last of "Supervised Learning" (Apr 11) - 4 more
- [ ] *Solution* of the text mining challenge - did you manage?
- [ ] *Bayesian Stats Rapid Review* - preview for this week's test
- [ ] *Lecture/practice:* Regression methods 

** TODO Solution: Text mining challenge

** TODO Bayesian Statistics Rapid Review - Deriving Bayes' Formula

1) We have a dataset with 100 emails, 20 of which are spam. What's the
   probability of spam, and what's it called?
   #+begin_quote
   The *prior* probability of spam, P(spam), is 20/100 = 0.2
   #+end_quote

2) In this case, what's the probability of ham (the complement of spam)?
   #+begin_quote
   The *prior* probability of ham, P(ham), is 80/100 = 1 - P(spam) = 0.8
   #+end_quote

3) The word "lottery" appears in 15 spam, and in 5 ham emails. What's
   the probability than an email containing "lottery" is spam?
   #+begin_quote
   The (conditional) *posterior* probability that an email with
   "lottery" is spam, P(spam|lottery), is 15/20 = 3/4 = 0.75.
   #+end_quote

4) How could you visualize this scenario?
   #+attr_html: :width 600px :float nil:
   #+caption: Source: Grokking Machine Learning, Serrano (Manning, 2021)
   [[../img/lottery.png]]

5) In this scenario, what is "lottery"?
   #+begin_quote
   The word "lottery" is the *event* that helps us make a better
   estimate of the probability.
   #+end_quote

6) How could you visualize the scenario diagrammatically?
   #+attr_html: :width 600px :float nil:
   [[../img/state_space.jpg]]

7) In this diagram, what does the term "state space" refer to?
   #+begin_quote
   The term "state space" refers to the states of the system:
   1. Before (prior to) the event ("lottery").
   2. After (posterior to) the event ("lottery").
   #+end_quote

8) What is =P(spam) \cap P(ham)=?
   #+begin_quote
   Spam and ham are mutually exclusive events:
   + =P(spam) \cap P(ham) = 0= (intersection)
   + =P(spam) \cup P(ham) = 1= (union)
   #+end_quote

9) How could you visualize all the possibilities of this state space?
   #+attr_html: :width 600px :float nil:
   #+caption: Source: Grokking Machine Learning, Serrano (Manning, 2021)
   [[../img/lottery1.png]]

10) How could you express the possibilities algebraically?[fn:1]
    #+begin_example

                     *-----------------*--> P("lottery" | spam) = 0.15
                 *-->| P(Spam)=1/5=0.2 |
                 |   *-----------------*--> P(no "lottery | spam) = 0.05
                 |
   P(msg) = 1 -->*
                 |
                 |   *-----------------*--> P("lottery" | ham) = 0.05
                 *-->| P(Ham)=4/5=0.8  |
                     *-----------------*--> P(no "lottery | spam) = 0.75
    #+end_example

11) How do you express the possibilities of two events happening at
    the same time?[fn:2]
    #+begin_example
                                            P("lottery" \cap spam) =
                     *-----------------*--> P("lottery" | spam) \cap P(spam)
                 *-->| P(Spam)=1/5=0.2 |
                 |   *-----------------*--> P(\not "lottery" \cap spam) =
                 |                          P(no "lottery | spam) \cap P(spam)
   P(msg) = 1 -->*
                 |                          P("lottery" \cap ham) =
                 |   *-----------------*--> P("lottery" | ham) \cap P(ham)
                 *-->| P(Ham)=4/5=0.8  |
                     *-----------------*--> P(\not "lottery" \cap spam) =
                                            P(no "lottery | spam) \cap P(spam)

    #+end_example

    The same tree with probabilities[fn:3]:
    #+attr_html: :width 600px :float nil:
    #+caption: Source: Grokking Machine Learning, Serrano (Manning, 2021)
    [[../img/lottery2.png]]

12) Of these four events, how many do we need to compute the posterior
    probability =P(spam|"lottery")=?
    #+begin_quote
    We only need to consider those events where the word "lottery"
    appears - the two two branches.
    #+end_quote
    #+attr_html: :width 600px :float nil:
    #+caption: Source: Grokking Machine Learning, Serrano (Manning, 2021)
    [[../img/lottery3.png]]

13) The probabilities no longer add up to one. What to do?
    #+begin_quote
    We need two numbers that are in the same relative ratio as 3/20
    and 1/20 but that add to one. That's the sum of each divided by
    their sum: (1/20 + 3/20) / (1/20+3/20) = 1. After some algebra,
    this leads straight to Bayes' formula.
    #+end_quote



** TODO Finish Naive Bayes SMS Message Spam Case Study

* Footnotes

[fn:1] These are conditional probabilities computed on the whole
dataset. We're for example computing the probability that a given spam
message out of 100 messages contains the word "lottery" as
=P("lottery"|spam)=.

[fn:2] For example, the probability that an email is spam and contains
the word "lottery" - that's the intersection of two events,
=P("lottery" \cap spam)=. To compute, use the product rule: =P(A \cap B)=P(A|B) \cap P(B)=.

[fn:3] The values are obtained by product, e.g. for the top leaf,
=P("lottery"|spam) = 3/4 * 1/5 = 3/20 = 0.15=

