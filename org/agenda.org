#+title: Agenda Notebook
#+author: Marcus Birkenkrahe
#+subtitle: DSC 305 - Machine Learning
#+SEQ_TODO: TODO NEXT IN_PROGRESS | DONE CANCELLED
#+startup: overview hideblocks indent entitiespretty
:PROPERTIES:
:header-args:C:      :main yes :includes <stdio.h> :results output :exports both
:header-args:python: :session *Python* :python python3 :results output :exports both
:header-args:R:      :session *R* :results graphics output file :exports both
:header-args:C++:    :main yes :includes <iostream> :results output :exports both
:END:
* Week 1: Overview
#+attr_html: :width 600px: 
#+caption: Salvador Dali, The Persistence of Memory (1931)
[[../img/cover.jpg]]

What inspired this painting by Dali? Is the answer a prediction? What
influences the prediction? Can the prediction be validated, and how?

#+begin_quote
- Yes, the answer to the question is a prediction informed by
  historical data.

- It is influenced by Dali's own statements, historical context, such
  as cultural events at the time, and Dali's own experiences.

- The prediction can be partially validated but not proven.

- "The soft watches are an unconscious symbol of the relativity of
  space and time [like in Einstein's theory of special relativity]."
  (Ad√©s)

- "The soft watches were inspired by the surrealist perception of a
  Camembert [cheese] melting in the sun." (Prigogine)
#+end_quote

- [X] Overview: Course content, grading, tools, book, etc.
- [X] On completing DataCamp assignments
- [X] Introduction to Machine Learning (no code)
- [ ] Updating a decision tree model (no code)

** DONE On completing DataCamp assignments

1. Watch the video first with full attention.
2. Write down immediate questions for later.
3. Build a notebook for DataCamp courses.
4. Recreate the individual steps in the video.
5. Create or import datasets.
6. Look up function definitions.
7. Solve the exercises outside of DataCamp.
8. If you loathe Emacs, use DataLab or Colab instead.
9. Don't ignore nagging questions that you have.
10. Start a journal to summarize what you've learnt.

** DONE Review

1. What is "Git"?
   #+begin_quote
   Git is version control software that supports the development and
   maintenance of (large) software projects, e.g. Linux, but also
   any sizeable R package development.
   #+end_quote

2. Which machine learning techniques will we not cover?
   #+begin_quote
   Machine learning techniques that use "black-box" models based on
   neural network architectures.
   #+end_quote

3. What is a "literate program"?
   #+begin_quote
   A program that is written primarily for humans, that includes
   documentation, code, and output, and that can be tangled into
   source code only, or woven into documentation only.
   #+end_quote

4. What is *confirmation bias*?
   #+begin_quote
   Confirmation bias is the tendency to search for, interpret, and
   remember information in a way that *confirms* one's pre-existing
   beliefs or hypothesis (systematic assumptions) while giving less
   consideration to evidence that contradicts them.

   A colloquial term for confirmation bias is "tunnel vision".
   #+end_quote

5. Give an example for confirmation bias?
   #+begin_quote
   Someone who believes that climate change is a hoax might focus on
   cold weather as proof that global warming isn't real while ignoring
   historical evidence to the contrary.

   Conversely, someone who believes in climate change might
   overemphasize a single extreme weather event like a hurricane, as
   proof of climate change while ignoring that individual events
   usually have multiple factors.
   #+end_quote

6. *Let's discuss:* What is a good *response* to confirmation bias?
   #+begin_quote
   1. Actively look for information that challenges your beliefs.
   2. Engage in civilized debate with those who disagree with you.
   3. Ask neutral questions - not "Why" but "What" and "How".
   4. Rely on multiple sources created over a longer period of time.
   5. Challenge your own views through systematic self-reflection.
   6. Balance data-driven, mathematical, and heuristic methods.
   7. Make yourself more aware of the relative meaning of objectivity.
   8. Take time out from your hobby horses.
   9. Avoid social media like the plague.
   10. Eat and sleep well.
   #+end_quote

7. What's a "captcha"?
   #+begin_quote
   "Captcha" stands for "Completely Automated Public Turing" test to
   tell computers and humans apart. They range from media-based (e.g.
   text/image recognition) to behavior-based (e.g. motion patterns).
   #+end_quote

8. What's OCR?
   #+begin_quote
   Many old books and newspapers were digitized using Optical
   Character Recognition (OCR) software. When OCR struggled with
   certain words or symbols, von Ahn's reCaptcha presented users with
   two words: One that the software had difficulty recognizing, and
   one with a known answer. If their response to the known word was
   correct, then their response to the unknown word was used to
   improve the digitization process - an example of both
   "crowdsourcing" (as a human process) and labeling (as a machine
   learning step). Check out von Ahn's own explanation of "human
   computation" ([[https://youtu.be/PQ-xzwj_p_4?si=vD6-OoHcIwMWHnmh][BigThink, 2014]]) - shared in the group chat.
   #+end_quote

* Week 2: Introduction to machine learning

- [ ] Extending and testing a decision tree model
- [ ] *Lecture*: Introduction to machine learning

** DONE Testing and improving a classification model (lab)

- This is an example of a classroom exercise that you can use to get
  bonus points (which you may need to balance bad test results or
  missing assignments).

- If you missed class, or if you did not finish in class, you can
  submit this by the deadline. If you submit it after the deadline, or
  if you submit incomplete solutions, you lose points.


** DONE Lesson Review/Test Preview:

1. Which of these is an optimization task for machine learning?
   #+begin_quote
   1) Improve energy use in an office
   2) Predict election outcomes
   3) Identify email spam
   4) Detect fraudulent transactions
   #+end_quote
   #+begin_quote
   While all options involve ML tasks like classification or
   prediction, optimization refers to improving a process or a
   resource allocation. Prediction and classification can *indirectly*
   help to improve processes but that's not the primary purpose.

   A *task* is something that you do directly. A *goal* is something that
   may be achieved indirectly by performing (many) tasks. A *strategy*
   is a process (usually of multiple steps) to achieve a goal.

   Example: Spam identification
   - Identifying whether an email is spam is a classification *task*.
   - Improving the time spent on processing emails is a *goal*.
   - Using ML to classify spam, refine models over time, and automate
     filtering to unclutter the inbox is a *strategy*.

   Example: Optimize energy use
   - Balance different energy sources with ML is a *task*.
   - Reduce overall energy costs while maintaining comfort is a *goal*.
   - Using ML to predict energy usage patterns, adapt settings in
     real-time, and balance power sources is a *strategy*.
   #+end_quote

2. What is a key limitation of ML models? Example?
   #+begin_quote
   Lack of flexibility. The model needs to be retrained, retuned and
   retested if new data appear, and it can not infer logical steps
   outside of its specific predictive or classification task.

   Retraining especially (depending on the data) can come at a high
   cost. An example are generative AI models (LLMs), or automated
   driving models with real-world edge cases not found in training
   data, or when sensors deliver poor data in real-time.
   #+end_quote

3. What's a major challenge when processing human language, and why?
   #+begin_quote
   The difficulty in interpreting audio and text data accurately.

   Why: Variability of spoken language (dialects, intonations, noise),
   temporal nature of data (time-series), multi-modality (emotion,
   emphasis, intonation), and lack of annotated data.
   #+end_quote
   
4. Which ethical issues with ML can you think of?
   #+begin_quote
   1) Privacy (training/test data)
   2) Bias (e.g. educational ML)
   3) Reliability in critical situations (e.g. medical ML)
   4) Social pressure (to adopt or not adopt AI)
   5) Copyright issues (e.g. public/private training data)
   6) Decision power (e.g. self-driving cars, military applications)
   7) Addiction and compulsion (to use AI instead of analog/manual)
   8) Cheating (e.g. students)
   9) Accountability (e.g. black-box methods like neural nets)
   10) Job displacement and economic inequality (AI replacing job)
   #+end_quote

5. Which data can and should be anonymized and is that relevant for
   ML?
   #+begin_quote
   All confidential data (e.g. financial, health, survey) should be
   anonymized. ML makes this difficult because it can use existing
   features to identify/reconstruct actors and data that were deleted.
   #+end_quote

6. Define supervised and unsupervised learning with an example each.
   #+begin_quote
   1) *Supervised ML:* Training a model on a labeled dataset - each
      input has a corresponding output. Model learns to map inputs to
      outputs, and can apply this to unknown inputs. *Example:* Medical
      diagnosis (= output) based on observed patient data (= input).

   2) *Unsupervised ML*: Training a model on an unlabeled dataset -
      input is a pattern or a grouping, desired output is a structure
      to the pattern or grouping. *Example*: Segmentation of customers
      into clusters with similar buying behavior (= output) based on
      customer purchasing behavior (= input).
   #+end_quote

7. When and why are datasets split in machine learning? Issues?
   #+begin_quote
   Create separate training data (to build the model) and test data
   (to evaluate the model). Issues include existing patterns (bias) in
   the data (lack of randomization), missing data, missing features.
   #+end_quote

8. What are the earliest historical roots of machine learning?
   #+begin_quote
   Galton's statistical modeling using linear regression - his
   presentation [[https://archive.org/details/1877GaltonTypicalLawsOfHeredity/mode/2up]["Typical Laws of Heredity" (1877)]].
   #+end_quote

9. Is there a limit to the features that can (or should) be used?
   #+begin_quote
   There is no theoretical limit, quite the opposite. 
   #+end_quote

10. What should you maximize, and what should you minimize: Sample or
    population?
    #+begin_quote
    You should maxmimize the sample (the data you're using), and you
    cannot manipulate the population because it's a real world fact.
    #+end_quote

* Week 3: Machine learning models overview

- [X] *Projects:* Enter your ideas now - sprint review: Fri 7-Feb
- [X] *Review*: DataCamp - Understanding Machine Learning Models
- [X] *Lecture*: Intro to ML with code along (continued)
- [X] *Test 2* is live - deadline Tuesday 4-Feb (100%).
- [X] The test does not include today's content.
- [X] Next up: R review

** Projects - sprint review coming soon
#+attr_html: :width 600px: 
[[../img/projects.png]]

** DataCamp review questions and answers (test 2)

1) What is the difference between *classification* and *regression*?
   Provide examples of each.
   #+begin_quote
   - *Classification* predicts a category, such as "Yes" or "No" (e.g.,
     "Is this email spam?").
   - *Regression* predicts a continuous value, such as a temperature or
     stock price (e.g., "What will the temperature be tomorrow?").
   #+end_quote

2) Which *data structures* are suited to classification and regression
   in R?
   #+begin_quote
   - *Categorical* values are best stored as *factor* vectors with *levels*.
   - *Continuous* values are best stored as *arrays* (like *numeric*
     vectors, matrices)
   - Both of these can be stored as *data frames*.
   #+end_quote

   #+begin_src R :results output
     class(matrix())
   #+end_src

   #+RESULTS:
   : [1] "matrix" "array" 

3) What if you want to predict a *label* but the *predictors* have all
   different lengths (e.g., because of missing observations)?
   #+begin_quote
   When predictors have different lengths or missing observations,
   common approaches include:
   1. *imputation* (filling in missing values using statistical methods,
      such as mean, median, or regression),
   2. using *algorithms* that can handle missing data directly (e.g.,
      decision trees: skipping missing values during training, or
      averaging at split during prediction),
   3. *transforming* the data to a fixed size (e.g., by aggregating or
      padding).
   4. *ignoring* if the information in the predictors is less relevant.
   #+end_quote

4) What is the role of a *confusion matrix* in evaluating a model's
   performance?
   #+begin_quote
   To understand the power of a model beyond accuracy score.
   #+end_quote

5) There are 3 red ('fraudulent') and 27 blue ('legitimate') dots in
   the picture. How does the confusion matrix look like?
   #+attr_html: :width 250px: 
   [[../img/fraud.png]]

   #+attr_html: :width 400px: 
   [[../img/confused.png]]
   
6) Where would "false positives" be found in the image, and which
   color would they have?
   #+begin_quote
   "False positive" = cases that were predicted to be fraudulent (pink
   area) but were actually legitimate (blue): blue dots at the bottom.
   #+end_quote

7) So is this model: Accurate? Sensitive? Specific?
   #+begin_quote
   - It's accurate: 28/30 points are correctly classified.
   - It's moderately sensitive of the transactions true nature (true
     pos/(true pos + false neg)): focus on left half of the matrix.
   - It's not very specific for identifying fraud. (true neg/(true
     neg + false pos)): focus on right half of the matrix.
   #+end_quote

8) Provide a scenario where either accuracy or sensitivity is
   important, but not both.
  #+begin_quote
  - *Accuracy*: all classes are equally important. Example: handwriting
    recognition system. Misclassification is equally important for all
    digits.
  - *Sensitivity*: identifying true positives is more important than
    minimizing false negatives. Example: medical diagnosis. Healthy
    individuals may be flagged for testing (false positives
  #+end_quote

9) What is *overfitting*, and how can it be identified in a model's
   performance?
   #+begin_quote
   Overfitting occurs when a model performs well on training data but
   poorly on testing data, indicating that it memorized the training
   data instead of generalizing.  This can be identified by testing
   the model on a separate dataset and evaluating its performance.
   #+end_quote

10) What are some common techniques for *dimensionality reduction*, and
    why are they useful in machine learning?
    #+begin_quote
    Common techniques include *removing* irrelevant features, *combining*
    correlated features (e.g., height and weight into BMI), or
    *collapsing* features into a single underlying factor.
    #+end_quote

** Model training, generalization, and evaluation

- *Everybody:* What did you learn in the last couple of sessions?

- Summarize:

  1) Models that use prediction to optimize energy consumption

  2) Difference between supervised and unsupervised models

  3) ML involves splitting data into training & test data

  4) Re-captchas and Duolingo as ML applications

  5) The history stats leading to ML (before 1990)

  6) Different types of supervised learning (regression,
     classifiction).

  7) Using unsupervised learning to identify clusters.

- What (if anything) surprised / amazed / shocked you?

  1) Why ML is not used that much in medicine.

  2) How unsupervised ML detects patterns in unlabeled datasets

  3) How easy all of this is!

** Sample answers

- What did you learn: Limits of machine learning
  
  1) ML models are... 
     #+begin_quote
     ...highly inflexible - strictly parametrized and subject to an
     immutable workflow.
     #+end_quote

  2) The ML workflow always includes:
     #+begin_quote
     1. data set preparation and analysis
     2. data transformation
     3. model training & customization
     4. model testing & evaluation
     #+end_quote
     
  3) Manual examples you saw included: 
     #+begin_quote
     Medical diagnosis (decision tree) and Nile data (workflow)
          #+end_quote

  4) =Nile= data were: 
     #+begin_quote
     1. stored in an external file (=write=)
     2. read into memory (=read.table=)
     3. transformed to a time-series (=ts=)
     4. modeled statistically (=summary=)
     5. modeled graphically (=boxplot=, =plot=)
     #+end_quote

  5) What remains:
     #+begin_quote
     Training & Generalization & Evaluation
     #+end_quote

- What surprised/amazed/shocked me:
  #+begin_quote
  1. So many new project ideas (check the Google Chat).
  2. AI changes the game somewhat but not much (MLOps).
  3. Most ML isn't generative AI but traditional stats.
  4. High attendance & classroom spirits despite the cold.
  #+end_quote

* Week 4: Modeling assignment, Deep Learning

** TODO Home assignment: Fitting a linear model to compute g 
#+attr_html: :width 400px: 
[[../img/newton.jpg]]

- Different ways to solve this problem
- Simplest: compute the trendline and read off the new points
- Provide plots in =ggplot= and/or base R
- Provide notebook in Org-mode or another format

* Week 5: R Review - ML models

- [X] *Test 2:* Counted late submissions bc of network isues
- [X] *Test 3* due Friday (max 100%)
- [ ] *DataCamp* assignments: I relaxed the deadlines
- [ ] *R Basics review:* vectors, factors, lists, data frames
- [ ] *R Basics review:* Managing data
- [ ] *R Basics review:* Exploring data

** TODO R Review: Data structures

- The DataCamp lessons & our classroom work require "Intermediate R".

- Download and work through: [[https://tinyurl.com/R-data-structures][tinyurl.com/R-data-structures]] (raw).

- Upload your result then let's review together.

** TODO R Review: Manage and explore

- Download and work through:  [[https://tinyurl.com/R-manage-practice][tinyurl.com/R-manage-practice]]

- Download and work through:  [[https://tinyurl.com/R-explore-practice][tinyurl.com/R-explore-practice]]

* Week 6: Machine Learning Models - kNN classification

- [ ] *Lecture*: Machine Learning Models
