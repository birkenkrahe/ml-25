#+TITLE: k-NEAREST-NEIGBORS
#+AUTHOR: Marcus Birkenkrahe
#+SUBTITLE: Supervised lazy learner classifiers
#+STARTUP: overview hideblocks indent inlineimages
#+OPTIONS: toc:nil num:nil ^:nil
#+PROPERTY: header-args:R :session *R* :results output :exports both :noweb yes
:REVEAL_PROPERTIES:
#+REVEAL_ROOT: https://cdn.jsdelivr.net/npm/reveal.js
#+REVEAL_REVEAL_JS_VERSION: 4
#+REVEAL_INIT_OPTIONS: transition: 'cube'
#+REVEAL_THEME: black
:END:
* k-nearest neighbors
#+attr_latex: :width 300px
#+attr_html: :width 400px
[[../img/4_knn.jpg]]

- Lecture notes in GitHub
- Source: Lantz (2019), chapter 3, pp. 65-89
- See also: last DataCamp assignment chapter 1

* What you will learn

- Concepts for "lazy learners" classifiers
- How to measure similarity using distance
- Demo: cancer classification using R

* What's the basic idea?

- The basic idea of this class of "lazy learners" is that things that
  are alike are likely to have properties that are alike.

- ML uses this principle to classify data by placing it in the same
  category as similar, or "nearest" neighbors.

- The choice of similarity measure and the choice of the parameter k
  are crucial and depend on problem and data.

* Nearest neighbor classification
#+attr_html: :width 400px
[[../img/4_darkrestaurant.png]]

- Classify unlabeled (unknown) examples
- Assign similar labeled (known) examples
- Human examples: reading, eating, meeting
- Simple but powerful methods with some limitations

- Remember our "unit" terminology for ML models?
  #+begin_quote
  1) Unit of observation, e.g. email/patients
  2) Examples or instances, e.g. message/patient sample
  3) Features: e.g. words/characteristics of message/patient
  #+end_quote

- With nearest-neighbor classification, computers apply a human-like
  ability to recall past experiences to make conclusions about current
  circumstances.

- Human examples:
  #+begin_quote
  - Reading: making sense of sentences and words by context.
  - Eating: "dark restaurants" where you are totally blind.
  - Meeting: making connections based on who's next to whom.
  #+end_quote

- Laziness of kNN:
  #+begin_quote
  kNN isn't actually "learning" anything: *there's no model* that
  improves with experience, and all data are kept *in memory* (unlike
  regression or decision trees that compute the "next step").
  #+end_quote

* Nearest-neighbor applications
#+attr_html: :width 400px
[[../img/4_nn_applications.jpg]]

- *Computer vision* applications
  #+begin_quote
  For example optical character recognition (OCR) and facial
  recognition in both still images and video - *Sports:* player
  classification/action recognition. Based on a set of criteria,
  player data are searched for belonging to a certain class, like
  "attack", "defense" etc.
  #+end_quote

- *Recommender systems*
  #+begin_quote
  Systems that predict whether a person will enjoy a movie or
  song. *Sports:* retrieving historically similar games. Similarity
  could be based on: Time to win, team composition, game location etc.
  #+end_quote

- *Genetic data pattern matching* 
  #+begin_quote
  Identifying patterns in genetic data to detect specific proteins or
  diseases. Similarity is based on how close together two genetic data
  points are in the "gene feature space" - more complicated measures
  need to be applied (because gene sequences vary in length or have
  gaps).
  #+end_quote

* Applicability
#+attr_html: :width 600px
#+caption: Source: stroke.org
[[../img/4_stroke.jpg]]

1. Features related in complex ways 
2. Similar items homogeneous 
3. "You know it when you see it"

c#+begin_quote
1. Well suited for classification tasks where relationships among the
   features and the target classes are *numerous*, *complicated*, or
   otherwise extremely *difficult to understand*, yet the items of
   similar class type tend to be fairly homogeneous.

2. E.g. a *stroke event*: unit of observation = person, example = a
   person with a problem, features: face, arms and speech
   symptoms. Note that we could also treat the "stroke prediction"
   with decision trees (giving a weight to each symptom).

3. If a concept is difficult to define, but *you know it when you see
   it*, then nearest neighbors might be appropriate. On the other hand,
   if the data is *noisy* and thus no clear distinction exists among the
   groups, nearest neighbor algorithms may struggle to identify the
   class *boundaries*.
#+end_quote

* Exercise: When to Apply k-Nearest Neighbors (kNN)?

For each scenario, choose whether kNN is *appropriate*, *not
appropriate*, or *needs modification*. Explain your choice.

1. *Spam Detection in Emails:* A company wants to classify emails as
   spam or not spam based on features like the presence of certain
   words, email length, and the number of recipients. The dataset has
   *millions of emails*, and new emails arrive *in real-time*.

   #+begin_quote
   *Not appropriate*: The dataset is too large, making kNN slow (all
   data in memory). Additionally, kNN does not perform well with
   high-dimensional *text data* ("curse of dimensionality", noise,
   cost). A model like *Naive Bayes* or a neural network would be
   better.
   #+end_quote

2. *Movie Recommendation System*: A streaming service wants to recommend
   movies to users based on their past viewing history. Each movie is
   represented by features like genre, average user rating, and
   duration. There are *several thousand* movies, and the system needs
   to suggest similar movies to users.

   #+begin_quote
   *Needs modification:* kNN can work for finding *similar movies*, but if
   the dataset is *too large*, it becomes inefficient. Approximate
   Nearest Neighbors (ANN - uses efficient data structures like
   hashing) or *Collaborative Filtering* (pre-selection) may be better.
   #+end_quote

3. *Medical Diagnosis Based on Patient Data*: A hospital wants to
   predict whether a patient has a disease based on their age, blood
   pressure, and cholesterol levels. The dataset contains *only a few
   thousand patient records*, and predictions do not need to be
   instant.

   #+begin_quote
   *Appropriate:* The dataset is *small enough* for kNN to work
   efficiently. The features (age, blood pressure, cholesterol) are
   *continuous* (choice of measure) and *low-dimensional*, making kNN a
   reasonable choice: feature space is not too large.
   #+end_quote

* The ~k~-NN algorithm
#+attr_html: :width 500px
[[../img/4_knn_cat_dog.png]]

1) Pick number of nearest neighbors *k*
2) Use *labeled* training data set
3) Identify k nearest records in test data
4) Assign class to unlabeled test instance

#+begin_quote
- k-NN uses information about a sample's *k* nearest neigbors to
  classify unlabeled examples
- *k* is the number of nearest neighbors (could be any number)
- After choosing k, use training set *labeled* by nominal categorical
  variables (the classes or groups to choose from)
- For each *unlabeled* record in test set, k-NN identifies similar
  records
- Unlabeled test instance is assigned the *majority* class
#+end_quote

* Workflow
#+attr_html: :width 500px
[[../img/4_knn_algorithm.png]]
#+begin_quote
Classification with the "trained" model:
- *Calculate* distances to all other points (records)
- *Rank* points according to k
- *Vote:* Put test record into majority class
#+end_quote
* Example: blind tasting
#+attr_html: :width 600px
[[../img/4_knn_food.png]]

#+begin_quote
- We want to predict/classify food features that we cannot see
  (sweetness and crunchiness) based on similarity to other foods.
- Prior to eating we recorded previously-tasted ingredients.
- We rated two features of each ingredient from 1 to 10.
- We labeled each ingredient as one of 3 food types.

This is just like the "workflow" example shown before - instead of
three *color classes*, we now have three *food type *classes*, protein,
fruit and vegetable.
#+end_quote

* Training: feature space
#+attr_html: :width 600px
[[../img/4_knn_plot.png]]

- Where is the feature space?
- What type of plot is this?
- What type of data are we processing?
- What data structures are appropriate?

#+begin_quote
- The k-NN algorithm treats the features as coordinates in a
  multi-dimensional *feature space*.
- Visualized in our example: *d=2* with a few more features added.
- What kind of plot is this? A *scatterplot*!
- To plot features, which are column vectors in our table, we use a
  *scatterplot* of crunchiness vs. sweetness
- Constructing the dataset in this way is part of the *"training"*:
  building a feature space of known, labeled (coordinated) features
  for classification of unlabeled (uncoordinated), unknown features.
- Our third feature, the *food type* is the class for classification.
#+end_quote

* Training: feature patterns
#+attr_html: :width 600px
[[../img/4_knn_patterns.png]]

- When plotting this in R, which data structures to choose?
- When we have this plot, are we done?
- Why is this phase called "training"?

#+begin_quote
The third feature is represented here as a grouping - when plotting
this in R, how would you represent it?

Answer: as a factor vector with three (nominal) levels, vegetables,
fruits, and proteins. In the plot, you could color the groups
according to this category (we'll plot an example later).

Similar types of food are grouped closely together:
- Vegetables are crunchy but not sweet
- Fruits are sweet and either crunchy or not crunchy
- Proteins are neither crunchy nor sweet

Question: could we also start with the food type, treat it as a
feature and classify whether the food is crunchy or sweet?
#+end_quote

* Testing: label examples
#+attr_html: :width 600px
[[../img/4_knn_tomato.png]]

- How can we apply kNN to the tomato?
- Which condition must the tomato fulfil to be used here?

#+begin_quote
- We are looking for the food type (class) most "similar" to tomatos
- Similarity is measured in our 2-dimensional feature space.
- Locating the tomato's nearest neighbors requires a *distance function*
- The tomato sits conveniently between our classes. Its sweetness and
  crunchiness must have been measured (without too much variability).

- A distance function measures the "similarity" between two instances
- Traditionally, k-NN uses *Euclidean distance*, measuring "by ruler"
- Other common measures: check out ~help(dist)~, which lists 6 different
  types.
#+end_quote
* Euclidean distance
#+attr_html: :width 600px
[[../img/4_knn_euclid.png]]

- $pp$, $q$: examples to be compared
- e.g. p=tomato, q=grape
- $1...n$: example features
- e.g. $1$ = sweetness, $2$ = crunchiness

#+attr_html: :width 600px
[[../img/4_knn_euclid1.png]]

- What would change if we had more than two features available?
- Would more features improve the performance of the kNN algorithm?
- Will more samples improve the performance of the kNN algorithm?

#+begin_quote
- The general formula allows for $n$-dimensional feature vectors.
- In our food example, $p$ corresponds to the tomato, and $q$ to any
  other labeled object, e.g. a grape, a green bean etc.
- The index corresponds to the recorded features with whom the
  examples were labeled: here, we only have d=2, sweetness and
  crunchiness.
- The last formula shows the computed Euclidean distance for the
  tomato and the green bean examples.

If we had more than two features available, each square root would
have three, not two, terms ($n = 3$).

Performance might be improved depending if the features are relevant
and discriminative - "color" will not help, but "water content" or
"saltiness" might.

Small sample size weakens most algorithms. Larger samples are good as
long as they don't obfuscate the patterns, don't crowd neighbors, or
add irrelevant data.
#+end_quote

* Calculate distances
#+attr_html: :width 700px
[[../img/4_knn_distances.png]]

- 1-NN: "The tomato is a fruit" (k=1)
- 2-NN: "The tomato is a fruit or a protein" (k=2)
- 3-NN: "The tomato is a fruit" (k=3)

#+begin_quote
To classify the tomato as a vegetable, protein, or fruit, we'll begin
by assigning the tomato the food type of its single nearest
neighbor. This is called 1-NN classification because k = 1. The *orange*
is the single nearest neighbor to the tomato, with a distance of
1.4. As orange is a fruit, the 1-NN algorithm would classify a tomato
as a fruit.

Using k=2 creates unclear decision boundaries. There is no winner
between orange and nuts - it's a 1:1 split (with orange closer), and
kNN cannot pick a winner.

If we use the k-NN algorithm with k = 3 instead, it performs a vote
among the three nearest neighbors: orange, grape, and nuts. Now,
because the majority class among these neighbors is fruit (two of the
three votes), the tomato again is classified as a *fruit*.
#+end_quote

* Underfitting vs. overfitting
#+attr_html: :width 600px
#+caption: Image source: Viktor Lavrenko, 2014
[[../img/4_knn_fit.png]]

Underfitting | Perfect fit | Overfitting

#+begin_quote
In the image (by Viktor Lavrenko, 2014), the "predictor" is the set of
features used for prediction of the trend line (in the regression
example), and for the prediction of the two classes.
- *Underfitting:* boundary ignores points. Points do not clearly fall on
  either side of the line. Both training and test data have large
  errors.
- *Perfect fit:* the line segments the classes clearly - only a couple
  of noisy examples contaminate it and can be identified by distance
  from the line.
- *Overfitting:* noise data dominate the classification. The test data
  error will be large while the training data error is small.
#+end_quote

* Choice of k
#+attr_html: :width 600px
[[../img/4_knn_k.png]]

- $k$ determines performance on future data
- Danger of underfitting or overfitting
- "Bias-variance" tradeoff

#+begin_quote
- *Underfitting:* boundaries do not capture many relationships well -
  high error rate = large k (large neighborhood) ignores samples
- *Overfitting*: class boundaries too small, local neighborhoods
  dominate.
- *"Bias-variance" tradeoff*: large k reduces the impact of variance
  (data spread) caused by noisy data but can bias the learner so that
  small but important patterns are ignored.
- *Very large k*: then the majority class would always dominate the
  voting regardless of the nearest neighbors.
- *Very small k*: now nearest neighbors dominate even if they are just
  noise - e.g. *mislabeled* data - big issue in NLP because of ambiguity
  or overlapping meanings - large k averages out context across more
  eamples.
#+end_quote
* Data preparation
#+attr_html: :width 400px
[[../img/4_minmax.png]]

min-max normalization

#+attr_html: :width 400px
[[../img/4_zscore.png]]

z-score standardization

#+begin_quote
- The *distance* formula depends on the range: if you add a feature with
  a large scale, it will dominate the results (e.g. "spiciness" via
  Scoville scale - differences are much greater than sweet/crunchy).
- *Rescale features* to equal scales with min-max normalization (all
  values in 0 to 1) = how far (0 to 100%) did the original value fall
  along the original data range?
- *z-score standardization:* how many standard deviations (distance from
  the mean) do data fall above or below the mean? Unlike the normal
  distribution, the values are unbounded.
- Rescaling must be applied to training and test data (tricky - might
  be out of range - you may not know the maximum or minimum).
#+end_quote
* Add some spiciness

- Imagine you are building a kNN classifier to predict whether a fruit
  is an apple or a banana based on two features:

  1. Sweetness (scale 1:10)

  2. Spiciness (measured in Scoville units, from 0 to 100,000)

  A few data points:
  | *Fruit*  | *Sweetness* | *Spiciness* |
  |--------+-----------+-----------|
  | Apple  |         7 |         0 |
  | Banana |         9 |         0 |
  | Chili  |         2 |    50,000 |

  Using *Euclidean* disstance, the feature with the larger scale
  (spiciness) will dominate the calculation. Classification will
  follow Scoville rather than sweetness units and the classifier is
  ineffective.

* Exercise: Normalize the data

Normalize the data using R: Afterwards, both features should have comparable
ranges.

1) Use the min-max standardization function to =normalize=.
2) Define the dataset (combining data on all scales).
3) Print the original dataset.
4) Apply =normalize= to the data and print it.

#+begin_src R
  normalize <- function(x) return((x-min(x))/(max(x)-min(x)))

  sweetness <- c(7,9,2)
  spiciness <- c(0,0,50000)

  (c(sweetness,spiciness)->data)
  normalize(data)
#+end_src

#+RESULTS:
: [1]     7     9     2     0     0 50000
: [1] 0.00014 0.00018 0.00004 0.00000 0.00000 1.00000

* Dummy coding
#+attr_html: :width 250px
[[../img/4_dummy.png]]

2-category (binary) variable (male, female)

#+attr_html: :width 300px
[[../img/4_dummy1.png]]

3-category variable (hot, medium, cold)

#+begin_quote
- Euclidean distance is not defined for nominal ~categorical~ data
  ("characters") - need to convert data to ~numeric~ format.
- *Dummy coding* = split category set in binary values, if the values
  are exclusive (like ~male~ vs ~female~).
- An n-category nominal feature can be dummy coded by creating binary
  indicator variables for n-1 levels of the feature.
- Example: knowing that ~hot~ and ~medium~ are both ~0~ means that ~cold~ is
  ~1~ - it it's not hot or medium, it must be cold.
- Dummy coded data always fall on 0 or 1, so there is no need for
  min-max normalization.
- Ordinal categorical data could be numbered: this works only if the
  steps between categories are equivalent, e.g. "lower", "middle",
  "upper" class - distance between them not equal.
- Why not always dummy code? Because for numerical data you'd have to
  bin them into categories (losing precision).
#+end_quote
* Why is k-NN "lazy learning"?
#+attr_html: :width 600px
[[../img/1_lantz_3.jpg]]

- k-NN has no generalization/abstraction
- Data are stored verbatim (rapid training)
- Test/production relies on training data (slow)

#+begin_quote
- *Abstraction* is minimal - no detail is suppressed.
- *Generalization* (imposing rules) is not there at all.
- The *training* consists in putting the data together.
- The *testing* uses all training data so it can be slow.
#+end_quote

* Rote learning
#+attr_html: :width 600px
[[../img/4_lazy.png]]

- *Instance-based* learners build no models.
- *Non-parametric* methods learn no parameters.
- *Rote learners* find natural bias-free patterns.

#+begin_quote
- Alternative name: instance-based learners

- No model is built, no parameters are learnt. Unlike *linear
  regression* (mathematical model is built). Unlike *Naive Bayes* which
  learns probabilities (= parameters) from the training data.

- Everything depends on choice of k and choice of *similarity* measure.

- Upside: the learner can find "natural" patterns rather than trying
  to fit the data into a biased and potentially flawed functional form.
#+end_quote

* Strengths and Weaknesses

| STRENGTHS            | WEAKNESSES            |
|----------------------+-----------------------|
| Simple and effective | No model              |
| No assumptions       | Selection of k        |
| Fast training        | Slow classification   |
| Natural, no bias     | Additional processing |

#+begin_quote
STRENGTHS:
- Simple and effective: Does not produce a model, limiting the ability
  to understand how the features are related to the class
- Makes no assumptions about the underlying data distribution:
  bias-free, favors natural patterns
- Fast training phase since generalization (finding rules) is bypassed.

WEAKNESSES:
- No model is being built (no mathematical analysis of the model)
- Requires selection of an appropriate k - dangers of over- or underfitting
- Slow classification phase because all training data instances are used
- Nominal features and missing data require additional processing,
  e.g. min-max normalization or z-score standardization
#+end_quote

* Summary

- k-nearest neighbors does no learning at all
- k-NN stores training data, matches test data to most similar records
  in training set using a distance function
- Unlabeled example is assigned neighbor's label
- Though simple, k-NN performs well for extremely complex tasks

* References

- ~4_knn.jpg~: Photo by Beth Macdonald on Unsplash.
- ~4_darkrestaurant.png~: [[https://www.nytimes.com/2007/07/22/travel/22surfacing.html][Patrons at the Whale Inside Dark Restaurant]].
- ~4_nn_applications.jpg~: Photo by George Prentzas on Unsplash.
- ~4_knn_cat_dog.png~, ~4_knn_algorithm.png~ - Christopher (Feb 2, 2021).

- Lantz (2019). Machine Learning with R (3e). Packt.
- Christopher (2021). K-Nearest Neighbor. [[https://medium.com/swlh/k-nearest-neighbor-ca2593d7a3c4][URL: medium.com]].
