#+TITLE: Multiple linear regression - Case study
#+AUTHOR: Marcus Birkenkrahe
#+SUBTITLE: Case Study - Predicting medical expenses
#+STARTUP: overview hideblocks indent inlineimages
#+OPTIONS: toc:nil num:nil ^:nil
#+PROPERTY: header-args:R :session *R* :results output :exports both :noweb yes
* Rationale

- Health insurance companies only make money if they collect more in
  fees than they spend on medical care to its beneficiaries.

- What do you think are profit margins in other industries?
  #+begin_quote

  #+end_quote

- Medical expenses are difficult to estimate because the conditions
  that are the most costly to treat are rare and seem random.

- Analysis goal: use patient data to forecast average medical expense
  for at-risk segments of the population (like smokers or obese).

- Image source: [[https://content.naic.org/sites/default/files/2021-Annual-Health-Insurance-Industry-Analysis-Report.pdf][US Health Insurance Industry Analysis Report 2021]]

* ML workflow

1) 
2) 
3) 
4) 
5) 

* Getting the data

- Fun fact: the firm that designed the USCB HQ also designed the Burj
  Khalifa (Dubai), the Sears Tower (Chicago) and One World Trade
  Center (NYC)

- The dataset contains 1,338 examples of beneficiaries enrolled in an
  insurance plan with patient features and total medical expenses
  charged to the insurance plan for the calendar year:
  1) ~age~: An integer indicating the age of the primary beneficiary
     (excluding those above 64 years, as they are generally covered by
     the government).
  2) ~sex~: The policy holder's gender: either ~male~ or ~female~.
  3) ~bmi~: The body mass index (BMI), which provides a sense of how over
     or underweight a person is relative to their height. BMI is equal
     to weight (in kilograms) divided by height (in meters) squared. An
     ideal BMI is within the range of 18.5 to 24.9.
  4) ~children~: An integer indicating the number of children/dependents
     covered by the insurance plan.
  5) ~smoker~: A "yes" or "no" categorical variable that indicates
     whether the insured regularly smokes tobacco.
  6) ~region~: The beneficiary's place of residence in the US, divided
     into four geographic regions: ~northeast~, ~southeast~, ~southwest~, or
     ~northwest~.

- Import the data from ~insurance.csv~ after checking the file online in
  GitHub: [[https://bit.ly/ml_insurance][bit.ly/ml_insurance]]:

  #+begin_example bash

  #+end_example

- Import the data with ~read.csv~ and save them to ~insurance~:
  #+begin_src R :results silent

  #+end_src

* Exploring the data: variables and distribution

- Exploring the data follows the old adage: data structure,
  statistical summary, overview visualization (numeric data),
  frequency check (categorical data).

- But this exploration is not an activity for its own sake: especially
  in the case of linear regression we need to check if the data
  conform to the minimum criteria (or else we can stop):
  
  1) 
  2) 
  3) 

- Display the ~data.frame~ structure:
  #+begin_src R

  #+end_src

- What is the model's dependent variable?
  #+begin_quote

  #+end_quote

- Linear regression does not require a normally distributed dependent
  variable but the model often fits better when this is true (why?[fn:1])

- To check distribution qualities quickly, we can summarize the stats:
  #+begin_src R

  #+end_src

- What do you observe?
  #+begin_quote

  #+end_quote

- We visualize the distribution (what's the best graph for that?):
  #+begin_src R :results graphics file :file ../img/6_hist.png
  
  #+end_src

- The graph shows that the majority of people have annual medical
  expenses below US$15,000. Knowing the graphs structural weakness
  ahead of time will help us improve the linear model later on.

* Exploring the data: correlation matrix

- The *correlation matrix* gives an overview of how the variables relate
  to one another: given a set of variables, it provides a correlation
  for each pairwise relationship.

- To create a correlation matrix, use the ~cor~ command - take a look at
  its arguments first:
  #+begin_src R

  #+end_src

- Let's build this up slowly: the default for ~y~ is only relevant if ~x~
  is a matrix: how is the dependent variable correlated *with itself*?
  #+begin_src R
    ## Just the dependent variable - formatted as matrix

  #+end_src

- This makes sense because:
  #+begin_src R

  #+end_src

- Now for all ~numeric~ variables:
  #+begin_src R

  #+end_src

- What do we learn?
  #+begin_quote

  #+end_quote

* Exploring the data: scatterplot matrix

- A /scatterplot matrix/ or /pair plot/ shows the relationship of each
  variable with every other as a graph.

- You can feed the whole dataframe into the generic ~plot~ function:
  #+begin_src R :results graphics file :file ../img/6_plot.png

  #+end_src

- However, ~plot~ does not distinguish between numeric and categorical
  variables, and a scatterplot is meaningless for the latter.

- An alternative is ~graphics::pairs~[fn:3]:
  #+begin_src R :results graphics file :file ../img/6_pairs.png

  #+end_src

- The intersection of each row and column holds the scatterplot of the
  variables indicated by the row and column pair: e.g. the plot in the
  2nd row and 2nd column shows ~age ~ bmi~ or "age" as a function of
  "bmi" - its transpose value shows ~bmi ~ age~.

- Do you notice any patterns in these plots?
  #+begin_quote

  #+end_quote

- The ~pairs.panels~ function in the ~psych~ package contains more
  information (you need to install ~psych~):
  #+begin_src R :results graphics file :file ../img/pairs_panels.png

  #+end_src

- What do you see?
  #+begin_quote

  #+end_quote

* Training a model on the data

- We use the generic ~lm~ function from ~stats~- check arguments:
  #+begin_src R

  #+end_src

- We use the "formula" syntax - the independent variables can *all* be
  included with the ~.~ operator: ~lm(dep ~ ., data)~ or individually with
  the ~+~ operator.

- Just like seen in the ~glm~ example (logistic regression), you can
  include /interactions/ between independent variables with the ~*~
  operator to model the combined effect of two or more features.

- The following model relates the six independent variables to the
  total medical ~expenses~:
  #+name: insurance_model
  #+begin_src R :results silent

  #+end_src

- To see the estimated \beta coefficients, print the model:
  #+begin_src R

  #+end_src

- The ~Intercept~ is the predicted value when the independent variables
  are zero (not realistic since living persons have BMI > 0, age > 0).

- The \beta coefficients indicate the estimated increase (slope) in
  expenses for an increase of one unit in each of the features,
  assuming all other values are held /constant/.

- For example: for each additional year of ~age~, we expect an average
  of ~256.8~ expense increase per year.

- The ~lm~ function automatically dummy-codes each ~factor~ type variable
  included, like ~sex~, ~smoker~ and ~region~ (split in four dummy variables).

- When adding dummy variables, one category is always left out as a
  reference category (e.g. ~sex=female~, ~region=northeast~): e.g. males
  have ~$131.4~ less medical expenses than females per year relatives to
  females[fn:4].

- Which ~region~ has the highest medical expenses?
  #+begin_quote

  #+end_quote

- In summary: old age, smoking and obesity can be linked to additional
  health issues, and additional family members may result in an
  increase. But how well is this model fitting the data?

* Evaluating model performance

- Why don't we use a confusion matrix?
  #+begin_quote

  #+end_quote

- So which function should we use?
  #+begin_src R

  #+end_src

- Explanation:

  1) 

  2) 

  3) 

  4) 

  5) 

* Excursion: z value and Pr(>|z|)

- What would be the null hypothesis for our prediction of insurance
  expenses?
  #+begin_quote

  #+end_quote

* Adding non-linear relationships

- To account for a non-linear relationship, we can add a higher order
  term treating the model as a polynomial.

- The additional \beta coefficient will capture the effect of the x^{2} term.

- Looking at the /loess curve/ (in the scatterplot matrix) which
  revealed non-linearity, ~age~ might be a good candidate for a
  nonlinear term. It is also already the strongest correlated
  independent variable so that its correction will be felt more
  strongly.

- Does this make sense? Consider the health pattern of aging: at the
  beginning and at the end of life, the need for (expensive) medical
  attention is greatest (if there's any need at all).

- In R, we simply create a new variable ~age2~ - this will add a feature
  vector and another column to our \beta coefficients matrix:
  #+begin_src R

  #+end_src

  #+RESULTS:

- When we build the model, we add both age variables to the formula,
  as in ~expenses ~ age + age2~, allowing ~lm~ to separate the terms.

* Converting numeric variable to binary indicator

- If a feature is not /cumulative/ but rather has has an effect only
  above a certain /threshold/, we would want this dynamical behavior to
  be modeled.

- BMI is such an /indicator/ variable: it is 1 (impactful) if the BMI is
  at least 30, and 0 (neglectable) if the BMI is less than 30. The
  associated \beta indicates the average net impact on expenses for
  individuals with ~bmi >= 30~ relative to those with ~bmi < 30~.

- R's ~ifelse~ function tests a condition for each element in a vector
  and returns a value accordingly - we use this to add another
  variable:
  #+begin_src R

  #+end_src

- In the ~insurance~ model, you can either replace the original ~bmi~ or
  add it depending on the type of impact of BMI:
  1) does the effect add to the impact of BMI?
  2) does the effect replace the impact of BMI?

- What is further statistical evidence if a variable should be
  included or not?
  #+begin_quote

  #+end_quote

- A quick check with ~bmi30~ shows that the p-value is still solid:
  #+begin_src R

  #+end_src

* Adding interaction effects

- Certain features could have a combined impact on the dependent
  variable - we can model this by their /interaction/.

- Smoking *and* obesity may be harmful separately (~+~) but in combination
  (~*~) they may be even more harmful: "Overweight smokers are ill more
  often than slim smokers".

- The ~*~ operator in the formula ~expenses ~ bmi30*smoker~ is shorthand
  for: ~expenses ~ bmi30 + smokeryes + bmi30:smokeryes~ - it includes
  the separate effects plus their interaction (~:~).

- The \beta coefficient of the interaction between ~bmi30~ and ~smokeryes~ is
  the increase of effectiveness of ~bmi30~ for a 1 unit increase in
  ~smokeryes~ and vice versa.

* The improved regression model

- We added a nonlinear term ~age2~ for ~age~, we created an indicator
  ~bmi30~ for obesity, and we specified an interaction between obesity
  and smoking (~bmi30*smoker~).

- We train the model as before with ~lm~ and include the new variables
  and the interaction term:
  #+begin_src R :results silent

  #+end_src

- Look at the model:
  #+begin_src R

  #+end_src

  #+RESULTS:

- Summarize the results:
  #+begin_src R

  #+end_src

  #+RESULTS:

- Interpret the new result:

  1)
     
  2)
     
  3) 

- Regression modeling makes strong assumptions on the underlying
  data. Before making inferences from the \beta coefficients, you need to
  run diagnostic tests to ensure that these assumptions have not been
  violated.

- These key assumptions are:
  1) multivariate normality (central limit theorem for multiple
     variables)
  2) little multicollinearity (relationship is not perfect)
  3) no autocorrelation (no periodicity in time)
  4) homoscedasticity (homogeneous noise in the relationship between
     independent and dependent variable)

- These assumptions are not relevant for numeric forecasting (model's
  worth is not based on truly capturing the underlying process)

* Making predictions with the improved regression model

- We use the model to predict the expenses of future enrollees on the
  health insurance plan.

- Apply the model to the original training data using ~predict~:
  #+begin_src R :results silent

  #+end_src

- Compute the correlation between predicted and actual costs:
  #+begin_src R

  #+end_src

  #+RESULTS:

- The model is highly accurate. Let's visualize this using ~plot~:
  #+begin_src R :results graphics file :file ../img/ins_mod2.png

  #+end_src

- In the plot, the line y = x shows the points/patients where
  predictions fall very close to the actual values.

- The points above the line are patients whose actual expenses were
  greater than expected, those below the line are those less than
  expected.

- To forecast for a smaller number of enrollees, you can create a
  dataframe on the fly. For example to estimate the insurance expenses
  for a 30-year old, overweight, male non-smoker with two children in
  the Northeast:
  #+begin_src R

  #+end_src

  #+RESULTS:

- For this beneficiary, the insurance company has to set its prices to
  no less than $6,000 or $500 per month to break even.

- How does this compare to a female with the same characteristics?
  #+begin_src R

  #+end_src

  #+RESULTS:

- The difference between these values is ~-496.769~ the estimated \beta for
  ~sexmale~: all else being equal, males are estimated to have about
  $500 less in expenses for the plan per year, all else being equal.

- The predicted expenses are a sum of each of the \beta values times their
  corresponding prediction settings, e.g. for the number of children
  the \beta is 678.6017. We can predict that reducing the children from 2
  to 0 (childless female), expenses will drop by 2\beta = 1,357.203:
  #+begin_src R

  #+end_src

  #+RESULTS:

- What if you have a lot of kids? Check out the result for 7 children:
  #+begin_src R

  #+end_src

- As a mother of 7, you are predicted to incur ca. $3500 more, and not
  (7/2)*$6,400=$22,400, so it's "cheap" to have a large family.

- Following similar steps for a number of additional customer
  segments, the insurance company could develop a profitable pricing
  structure for various demographics.


