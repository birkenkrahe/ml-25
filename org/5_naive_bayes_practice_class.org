#+TITLE: Supervised Learning with Naive Bayes
#+AUTHOR: [yourname] [pledge]
#+SUBTITLE: Case Study - Filtering mobile phone spam
#+STARTUP: overview hideblocks indent inlineimages
#+OPTIONS: toc:nil num:nil ^:nil
#+PROPERTY: header-args:R :session *R* :results output :exports both :noweb yes
* README

- This lecture and practice follows the case developed by Lantz (2019)
  and the Bag-of-Words method detailed in Kwartler (2017).

- We use the ~tm~ R package for text mining originally developed by
  Feinerer (2008).

* DONE Collecting the data

- Take a look at the raw file to check if there's a header:
  [[https://bit.ly/sms_spam_csv][bit.ly/sms_spam_csv]]

- Import the CSV data and save them to a data frame ~sms_raw~. Do not
  automatically convert ~character~ to ~factor~ vectors. Use the
  appropriate function arguments:
  #+begin_src R :results silent
    ## save CSV data as data frame sms_raw
    sms_raw <- read.csv(file="https://bit.ly/sms_spam_csv",
                        header=TRUE,
                        stringsAsFactors=FALSE)
  #+end_src

- Check that the data frame was loaded:
  #+begin_src R
    ls()
  #+end_src

  #+RESULTS:
  :  [1] "bar"                "convert_counts"     "foo"               
  :  [4] "ham"                "loaded"             "m"                 
  :  [7] "sms_corpus"         "sms_corpus_clean"   "sms_dtm"           
  : [10] "sms_dtm_freq_test"  "sms_dtm_freq_train" "sms_dtm_test"      
  : [13] "sms_dtm_train"      "sms_dtm2"           "sms_freq_words"    
  : [16] "sms_raw"            "sms_test"           "sms_test_labels"   
  : [19] "sms_train"          "sms_train_labels"   "spam"              
  : [22] "string"             "x"                  "y"

* DONE Exploring the data

- Check the data structure:
  #+begin_src R
    ## check the data structure
    str(sms_raw)
    head(sms_raw$text,10)
  #+end_src

  #+RESULTS:
  #+begin_example
  'data.frame':	5559 obs. of  2 variables:
   $ type: chr  "ham" "ham" "ham" "spam" ...
   $ text: chr  "Hope you are having a good week. Just checking in" "K..give back my thanks." "Am also doing in cbe only. But have to pay." "complimentary 4 STAR Ibiza Holiday or £10,000 cash needs your URGENT collection. 09066364349 NOW from Landline "| __truncated__ ...
  [1] "Hope you are having a good week. Just checking in"                                                                                                                
   [2] "K..give back my thanks."                                                                                                                                          
   [3] "Am also doing in cbe only. But have to pay."                                                                                                                      
   [4] "complimentary 4 STAR Ibiza Holiday or £10,000 cash needs your URGENT collection. 09066364349 NOW from Landline not to lose out! Box434SK38WP150PPM18+"            
   [5] "okmail: Dear Dave this is your final notice to collect your 4* Tenerife Holiday or #5000 CASH award! Call 09061743806 from landline. TCs SAE Box326 CW25WX 150ppm"
   [6] "Aiya we discuss later lar... Pick u up at 4 is it?"                                                                                                               
   [7] "Are you this much buzy"                                                                                                                                           
   [8] "Please ask mummy to call father"                                                                                                                                  
   [9] "Marvel Mobile Play the official Ultimate Spider-man game (£4.50) on ur mobile right now. Text SPIDER to 83338 for the game & we ll send u a FREE 8Ball wallpaper" 
  [10] "fyi I'm at usf now, swing by the room whenever"
  #+end_example

- Convert the spam vs. ham label to a ~factor~ and confirm the
  conversion:
  #+begin_src R
    ## convert class character vector to factor
    factor(sms_raw$type) -> sms_raw$type
    ## confirm conversion to factor
    is.factor(sms_raw$type)
    str(sms_raw)
  #+end_src

  #+RESULTS:
  : [1] TRUE
  : 'data.frame':	5559 obs. of  2 variables:
  :  $ type: Factor w/ 2 levels "ham","spam": 1 1 1 2 2 1 1 1 2 1 ...
  :  $ text: chr  "Hope you are having a good week. Just checking in" "K..give back my thanks." "Am also doing in cbe only. But have to pay." "complimentary 4 STAR Ibiza Holiday or £10,000 cash needs your URGENT collection. 09066364349 NOW from Landline "| __truncated__ ...


- Examine the frequency of spam vs. ham messages in the dataset:
  #+begin_src R
    ## examine frequency of spam vs ham
    prop.table(table(sms_raw$type))
  #+end_src

  #+RESULTS:
  : 
  :       ham      spam 
  : 0.8656233 0.1343767

* DONE Getting the ~tm~ R package

- Install and load ~tm~ (load it from the terminal if you haven't set
  ~options()$repos~ in your ~~/.Rprofile~ file). This is an actively
  developed package so re-installation will never do any harm:
  #+begin_src R
    ## install tm
    #install.packages("tm")
    ## load tm
    library(tm)
  #+end_src

  #+RESULTS:
  : Loading required package: NLP

- Check that the package has been loaded and look at the methods
  (functions) included in ~tm~:
  #+begin_src R
    ## check package has been loaded
    search()
    ## list functions in tm
    ls('package:tm')
    ## to see the test datasets
    data(package="tm")
  #+end_src

  #+RESULTS:
  #+begin_example
  [1] ".GlobalEnv"        "package:tm"        "package:NLP"      
   [4] "ESSR"              "package:stats"     "package:graphics" 
   [7] "package:grDevices" "package:utils"     "package:datasets" 
  [10] "package:methods"   "Autoloads"         "package:base"
  [1] "as.DocumentTermMatrix"   "as.TermDocumentMatrix"  
   [3] "as.VCorpus"              "Boost_tokenizer"        
   [5] "content_transformer"     "Corpus"                 
   [7] "DataframeSource"         "DirSource"              
   [9] "Docs"                    "DocumentTermMatrix"     
  [11] "DublinCore"              "DublinCore<-"           
  [13] "eoi"                     "findAssocs"             
  [15] "findFreqTerms"           "findMostFreqTerms"      
  [17] "FunctionGenerator"       "getElem"                
  [19] "getMeta"                 "getReaders"             
  [21] "getSources"              "getTokenizers"          
  [23] "getTransformations"      "Heaps_plot"             
  [25] "inspect"                 "MC_tokenizer"           
  [27] "nDocs"                   "nTerms"                 
  [29] "PCorpus"                 "pGetElem"               
  [31] "PlainTextDocument"       "read_dtm_Blei_et_al"    
  [33] "read_dtm_MC"             "readDataframe"          
  [35] "readDOC"                 "reader"                 
  [37] "readPDF"                 "readPlain"              
  [39] "readRCV1"                "readRCV1asPlain"        
  [41] "readReut21578XML"        "readReut21578XMLasPlain"
  [43] "readTagged"              "readXML"                
  [45] "removeNumbers"           "removePunctuation"      
  [47] "removeSparseTerms"       "removeWords"            
  [49] "scan_tokenizer"          "SimpleCorpus"           
  [51] "SimpleSource"            "stemCompletion"         
  [53] "stemDocument"            "stepNext"               
  [55] "stopwords"               "stripWhitespace"        
  [57] "TermDocumentMatrix"      "termFreq"               
  [59] "Terms"                   "tm_filter"              
  [61] "tm_index"                "tm_map"                 
  [63] "tm_parLapply"            "tm_parLapply_engine"    
  [65] "tm_reduce"               "tm_term_score"          
  [67] "URISource"               "VCorpus"                
  [69] "VectorSource"            "weightBin"              
  [71] "WeightFunction"          "weightSMART"            
  [73] "weightTf"                "weightTfIdf"            
  [75] "writeCorpus"             "XMLSource"              
  [77] "XMLTextDocument"         "Zipf_plot"              
  [79] "ZipSource"
  Data sets in package ‘tm’:

  acq                     50 Exemplary News Articles from the
                          Reuters-21578 Data Set of Topic acq
  crude                   20 Exemplary News Articles from the
                          Reuters-21578 Data Set of Topic crude
  #+end_example

* DONE Building a document text corpus

- Three steps lead from a data frame with text to a corpus:
  1) Isolate the text vector
  2) Turn the vector into a source
  3) Turn the source into a corpus
  4) Check that the corpus is there
  #+begin_src R
    str(sms_raw)
    VCorpus(VectorSource(sms_raw$text)) -> sms_corpus
    ls()
  #+end_src

  #+RESULTS:
  #+begin_example
  'data.frame':	5559 obs. of  2 variables:
   $ type: Factor w/ 2 levels "ham","spam": 1 1 1 2 2 1 1 1 2 1 ...
   $ text: chr  "Hope you are having a good week. Just checking in" "K..give back my thanks." "Am also doing in cbe only. But have to pay." "complimentary 4 STAR Ibiza Holiday or £10,000 cash needs your URGENT collection. 09066364349 NOW from Landline "| __truncated__ ...
  [1] "bar"                "convert_counts"     "foo"               
   [4] "ham"                "loaded"             "m"                 
   [7] "sms_corpus"         "sms_corpus_clean"   "sms_dtm"           
  [10] "sms_dtm_freq_test"  "sms_dtm_freq_train" "sms_dtm_test"      
  [13] "sms_dtm_train"      "sms_dtm2"           "sms_freq_words"    
  [16] "sms_raw"            "sms_test"           "sms_test_labels"   
  [19] "sms_train"          "sms_train_labels"   "spam"              
  [22] "string"             "x"                  "y"
  #+end_example

- The ~VCorpus~ function creates a volatile, in-memory list that is
  not permanent (not for writing to an external database):
  #+begin_src R
    inspect(sms_corpus[[1]])
  #+end_src

  #+RESULTS:
  : <<PlainTextDocument>>
  : Metadata:  7
  : Content:  chars: 49
  : 
  : Hope you are having a good week. Just checking in

* DONE Explore the text corpus

- The corpus is a list (~class~ will not reveal this but ~typeof~ will):
  #+begin_src R
    class(sms_corpus)
    typeof(sms_corpus)
  #+end_src

  #+RESULTS:
  : [1] "VCorpus" "Corpus"
  : [1] "list"

- You can see its content element-wise using list indexing. For
  example for message no. 999, ~tm::inspect~ returns meta data + content:
  #+begin_src R
    inspect(sms_corpus[[5559]])
  #+end_src

  #+RESULTS:
  : <<PlainTextDocument>>
  : Metadata:  7
  : Content:  chars: 31
  : 
  : Shall call now dear having food

- ~NLP::content~ returns just the content, but you can also use ~[[~ to
  extract a message:
  #+begin_src R
    sms_corpus[[1]][1] # with the $content label
    content(sms_corpus[[1]])
    as.character(sms_corpus[[1]])
  #+end_src

  #+RESULTS:
  : $content
  : [1] "Hope you are having a good week. Just checking in"
  : [1] "Hope you are having a good week. Just checking in"
  : [1] "Hope you are having a good week. Just checking in"

- ~content~ is in the ~NLP~ package:  
  #+begin_src R
    environment(content)
  #+end_src

  #+RESULTS:
  : <environment: namespace:NLP>

- There is a ~meta~ function that returns all the meta data:
  #+begin_src R
    meta(sms_corpus)
    meta(sms_corpus[[25]])
    meta(sms_corpus[[25]])[2]    
  #+end_src  

  #+RESULTS:
  #+begin_example
  data frame with 0 columns and 5559 rows
  author       : character(0)
    datetimestamp: 2025-04-01 21:56:54
    description  : character(0)
    heading      : character(0)
    id           : 25
    language     : en
    origin       : character(0)
  $datetimestamp
  [1] "2025-04-01 21:56:54 GMT"
  #+end_example

- To see several list elements at once, ~lapply~ will apply its ~FUN~
  argument to all ~list~ members - for the first three messages:
  #+begin_src R
    lapply(X=sms_corpus[1:3], FUN=as.character)
  #+end_src

  #+RESULTS:
  : $`1`
  : [1] "Hope you are having a good week. Just checking in"
  : 
  : $`2`
  : [1] "K..give back my thanks."
  : 
  : $`3`
  : [1] "Am also doing in cbe only. But have to pay."

* DONE Cleaning the text corpus: lower case, numbers

- Transformation of the whole corpus is done with the ~tm_map~ function,
  which accepts a corpus and a function as an argument - check that:
  #+begin_src R
    args(tm_map)
  #+end_src

  #+RESULTS:
  : function (x, FUN, ...) 
  : NULL

- To transform words to lower case, we use ~base::tolower~
  #+begin_src R
    tolower("WHY ARE YOU YELLING AT ME!")
  #+end_src

  #+RESULTS:
  : [1] "why are you yelling at me!"

- Check where ~tolower~ is at home:
  #+begin_src R
    environment(tolower)
    search()
  #+end_src

  #+RESULTS:
  : <environment: namespace:base>
  : [1] ".GlobalEnv"        "package:tm"        "package:NLP"       "ESSR"              "package:stats"    
  :  [6] "package:graphics"  "package:grDevices" "package:utils"     "package:datasets"  "package:methods"  
  : [11] "Autoloads"         "package:base"

- Since ~tolower~ is not in ~tm~, we need to wrap it in another function,
  ~tm::content_transformer~:
  #+begin_src R :results ourput
    tm_map(x = sms_corpus,
           FUN = content_transformer(tolower)) -> sms_corpus_clean
    ls()
  #+end_src

  #+RESULTS:
  : [1] "sms_corpus"       "sms_corpus_clean" "sms_raw"

- Let's check that the transformation worked: print the ~content~ of the
  first message from the original and the transformed corpus:
  #+begin_src R
    content(sms_corpus[[1]])  ## before cleaning
    content(sms_corpus_clean[[1]])  ## before cleaning    
    # after cleaning
  #+end_src

  #+RESULTS:
  : [1] "Hope you are having a good week. Just checking in"
  : [1] "hope you are having a good week. just checking in"

- To remove numbers from the SMS messages, use ~tm::removeNumbers~ on
  the new corpus object:
  #+begin_src R :results silent
    tm_map(x = sms_corpus_clean,
           FUN = removeNumbers) -> sms_corpus_clean
  #+end_src

- Compare the ~content~ of the original and transformed corpus for message 4:
  #+begin_src R
    content(sms_corpus[[4]])  ## before cleaning
    content(sms_corpus_clean[[4]])  ## before cleaning    
  #+end_src

  #+RESULTS:
  : [1] "complimentary 4 STAR Ibiza Holiday or £10,000 cash needs your URGENT collection. 09066364349 NOW from Landline not to lose out! Box434SK38WP150PPM18+"
  : [1] "complimentary  star ibiza holiday or £, cash needs your urgent collection.  now from landline not to lose out! boxskwpppm+"

- To see all ~tm~ functions that can be used with ~tm_map~, check the *help*
  for ~getTransformations~.

* DONE Removing stopwords and punctuation

- The ~tm~ package provides a ~stopwords~ function to access various sets
  of stop words from different languages. Check its arguments.
  #+begin_src R
    args(stopwords)
  #+end_src

  #+RESULTS:
  : function (kind = "en") 
  : NULL

- Which language contains the most stopwords?  Compare the ~length~ of
  ~english~, ~spanish~ and ~german~ ~tm::stopword~ dictionaries:
  #+begin_src R
    length(stopwords("english"))
    length(stopwords("spanish"))    
    length(stopwords("german"))
    length(stopwords("french"))        
  #+end_src

  #+RESULTS:
  : [1] 174
  : [1] 308
  : [1] 231
  : [1] 164

- To apply ~stopwords~ to the corpus, run ~removeWords~ on it. The
  ~stopwords~ function is an additional parameter (cp. ~args(tm_map)~):
  #+begin_src R
    tm_map(x=sms_corpus_clean,
           FUN=removeWords,
           c(stopwords("en"),"just")) -> sms_corpus_clean  # add a new stopword
  #+end_src

- Compare the ~content~ of the first message of the original and the
  cleaned corpus:
  #+begin_src R
    content(sms_corpus[[1]])  ## before cleaning
    content(sms_corpus_clean[[1]])  ## before cleaning    
  #+end_src

  #+RESULTS:
  : [1] "Hope you are having a good week. Just checking in"
  : [1] "hope     good week.  checking "

- Now remove the punctuation with ~removePunctuation~, save the result
  in a new ~sms_corpus_clean~ object, and compare before/after for
  message 16 :
  #+begin_src R
    tm_map(x=sms_corpus_clean, FUN=removePunctuation) -> sms_corpus_clean
    content(sms_corpus[[16]])  ## before cleaning
    content(sms_corpus_clean[[16]])  ## before cleaning    
  #+end_src

  #+RESULTS:
  : [1] "Ha ha cool cool chikku chikku:-):-DB-)"
  : [1] "ha ha cool cool chikku chikkudb"

- There are subtleties here: e.g. ~removePunctuation~ strips punctuation
  characters completely, with unintended consequences:
  #+begin_src R
    removePunctuation("hello...world")
  #+end_src

  #+RESULTS:
  : [1] "helloworld"

* DONE Word stemming with ~SnowballC~

- Word stemming involves reducing words to their root form. It reduces
  words like "learning", "learned", "learns" to "learn".

- In this way, the classifier does not have to learn a pattern for
  each variant of what is semantically the same feature.

- ~tm~ integrates word-stemming with the ~SnowballC~ package which needs
  to be installed separately, alas. Load the package and check its
  content:
  #+begin_src R
    install.packages("SnowballC")
    library(SnowballC)
  #+end_src

  #+RESULTS:
  #+begin_example
  Installing package into ‘/home/marcus/R/x86_64-pc-linux-gnu-library/4.1’
  (as ‘lib’ is unspecified)
  trying URL 'https://cloud.r-project.org/src/contrib/SnowballC_0.7.1.tar.gz'
  Content type 'application/x-gzip' length 405547 bytes (396 KB)
  ==================================================
  downloaded 396 KB

  ,* installing *source* package ‘SnowballC’ ...
  ,** package ‘SnowballC’ successfully unpacked and MD5 sums checked
  ,** using staged installation
  ,** libs
  gcc -I"/usr/share/R/include" -DNDEBUG      -fpic  -g -O2 -ffile-prefix-map=/build/r-base-4A2Reg/r-base-4.1.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c SnowballC_init.c -o SnowballC_init.o
  gcc -I"/usr/share/R/include" -DNDEBUG      -fpic  -g -O2 -ffile-prefix-map=/build/r-base-4A2Reg/r-base-4.1.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c api.c -o api.o
  gcc -I"/usr/share/R/include" -DNDEBUG      -fpic  -g -O2 -ffile-prefix-map=/build/r-base-4A2Reg/r-base-4.1.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c libstemmer_utf8.c -o libstemmer_utf8.o
  gcc -I"/usr/share/R/include" -DNDEBUG      -fpic  -g -O2 -ffile-prefix-map=/build/r-base-4A2Reg/r-base-4.1.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c stem.c -o stem.o
  gcc -I"/usr/share/R/include" -DNDEBUG      -fpic  -g -O2 -ffile-prefix-map=/build/r-base-4A2Reg/r-base-4.1.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c stem_UTF_8_arabic.c -o stem_UTF_8_arabic.o
  gcc -I"/usr/share/R/include" -DNDEBUG      -fpic  -g -O2 -ffile-prefix-map=/build/r-base-4A2Reg/r-base-4.1.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c stem_UTF_8_basque.c -o stem_UTF_8_basque.o
  gcc -I"/usr/share/R/include" -DNDEBUG      -fpic  -g -O2 -ffile-prefix-map=/build/r-base-4A2Reg/r-base-4.1.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c stem_UTF_8_catalan.c -o stem_UTF_8_catalan.o
  gcc -I"/usr/share/R/include" -DNDEBUG      -fpic  -g -O2 -ffile-prefix-map=/build/r-base-4A2Reg/r-base-4.1.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c stem_UTF_8_danish.c -o stem_UTF_8_danish.o
  gcc -I"/usr/share/R/include" -DNDEBUG      -fpic  -g -O2 -ffile-prefix-map=/build/r-base-4A2Reg/r-base-4.1.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c stem_UTF_8_dutch.c -o stem_UTF_8_dutch.o
  gcc -I"/usr/share/R/include" -DNDEBUG      -fpic  -g -O2 -ffile-prefix-map=/build/r-base-4A2Reg/r-base-4.1.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c stem_UTF_8_english.c -o stem_UTF_8_english.o
  gcc -I"/usr/share/R/include" -DNDEBUG      -fpic  -g -O2 -ffile-prefix-map=/build/r-base-4A2Reg/r-base-4.1.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c stem_UTF_8_finnish.c -o stem_UTF_8_finnish.o
  gcc -I"/usr/share/R/include" -DNDEBUG      -fpic  -g -O2 -ffile-prefix-map=/build/r-base-4A2Reg/r-base-4.1.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c stem_UTF_8_french.c -o stem_UTF_8_french.o
  gcc -I"/usr/share/R/include" -DNDEBUG      -fpic  -g -O2 -ffile-prefix-map=/build/r-base-4A2Reg/r-base-4.1.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c stem_UTF_8_german.c -o stem_UTF_8_german.o
  gcc -I"/usr/share/R/include" -DNDEBUG      -fpic  -g -O2 -ffile-prefix-map=/build/r-base-4A2Reg/r-base-4.1.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c stem_UTF_8_greek.c -o stem_UTF_8_greek.o
  gcc -I"/usr/share/R/include" -DNDEBUG      -fpic  -g -O2 -ffile-prefix-map=/build/r-base-4A2Reg/r-base-4.1.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c stem_UTF_8_hindi.c -o stem_UTF_8_hindi.o
  gcc -I"/usr/share/R/include" -DNDEBUG      -fpic  -g -O2 -ffile-prefix-map=/build/r-base-4A2Reg/r-base-4.1.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c stem_UTF_8_hungarian.c -o stem_UTF_8_hungarian.o
  gcc -I"/usr/share/R/include" -DNDEBUG      -fpic  -g -O2 -ffile-prefix-map=/build/r-base-4A2Reg/r-base-4.1.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c stem_UTF_8_indonesian.c -o stem_UTF_8_indonesian.o
  gcc -I"/usr/share/R/include" -DNDEBUG      -fpic  -g -O2 -ffile-prefix-map=/build/r-base-4A2Reg/r-base-4.1.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c stem_UTF_8_irish.c -o stem_UTF_8_irish.o
  gcc -I"/usr/share/R/include" -DNDEBUG      -fpic  -g -O2 -ffile-prefix-map=/build/r-base-4A2Reg/r-base-4.1.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c stem_UTF_8_italian.c -o stem_UTF_8_italian.o
  gcc -I"/usr/share/R/include" -DNDEBUG      -fpic  -g -O2 -ffile-prefix-map=/build/r-base-4A2Reg/r-base-4.1.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c stem_UTF_8_lithuanian.c -o stem_UTF_8_lithuanian.o
  gcc -I"/usr/share/R/include" -DNDEBUG      -fpic  -g -O2 -ffile-prefix-map=/build/r-base-4A2Reg/r-base-4.1.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c stem_UTF_8_nepali.c -o stem_UTF_8_nepali.o
  gcc -I"/usr/share/R/include" -DNDEBUG      -fpic  -g -O2 -ffile-prefix-map=/build/r-base-4A2Reg/r-base-4.1.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c stem_UTF_8_norwegian.c -o stem_UTF_8_norwegian.o
  gcc -I"/usr/share/R/include" -DNDEBUG      -fpic  -g -O2 -ffile-prefix-map=/build/r-base-4A2Reg/r-base-4.1.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c stem_UTF_8_porter.c -o stem_UTF_8_porter.o
  gcc -I"/usr/share/R/include" -DNDEBUG      -fpic  -g -O2 -ffile-prefix-map=/build/r-base-4A2Reg/r-base-4.1.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c stem_UTF_8_portuguese.c -o stem_UTF_8_portuguese.o
  gcc -I"/usr/share/R/include" -DNDEBUG      -fpic  -g -O2 -ffile-prefix-map=/build/r-base-4A2Reg/r-base-4.1.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c stem_UTF_8_romanian.c -o stem_UTF_8_romanian.o
  gcc -I"/usr/share/R/include" -DNDEBUG      -fpic  -g -O2 -ffile-prefix-map=/build/r-base-4A2Reg/r-base-4.1.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c stem_UTF_8_russian.c -o stem_UTF_8_russian.o
  gcc -I"/usr/share/R/include" -DNDEBUG      -fpic  -g -O2 -ffile-prefix-map=/build/r-base-4A2Reg/r-base-4.1.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c stem_UTF_8_spanish.c -o stem_UTF_8_spanish.o
  gcc -I"/usr/share/R/include" -DNDEBUG      -fpic  -g -O2 -ffile-prefix-map=/build/r-base-4A2Reg/r-base-4.1.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c stem_UTF_8_swedish.c -o stem_UTF_8_swedish.o
  gcc -I"/usr/share/R/include" -DNDEBUG      -fpic  -g -O2 -ffile-prefix-map=/build/r-base-4A2Reg/r-base-4.1.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c stem_UTF_8_tamil.c -o stem_UTF_8_tamil.o
  gcc -I"/usr/share/R/include" -DNDEBUG      -fpic  -g -O2 -ffile-prefix-map=/build/r-base-4A2Reg/r-base-4.1.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c stem_UTF_8_turkish.c -o stem_UTF_8_turkish.o
  gcc -I"/usr/share/R/include" -DNDEBUG      -fpic  -g -O2 -ffile-prefix-map=/build/r-base-4A2Reg/r-base-4.1.2=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c utilities.c -o utilities.o
  gcc -shared -L/usr/lib/R/lib -Wl,-Bsymbolic-functions -flto=auto -ffat-lto-objects -flto=auto -Wl,-z,relro -o SnowballC.so SnowballC_init.o api.o libstemmer_utf8.o stem.o stem_UTF_8_arabic.o stem_UTF_8_basque.o stem_UTF_8_catalan.o stem_UTF_8_danish.o stem_UTF_8_dutch.o stem_UTF_8_english.o stem_UTF_8_finnish.o stem_UTF_8_french.o stem_UTF_8_german.o stem_UTF_8_greek.o stem_UTF_8_hindi.o stem_UTF_8_hungarian.o stem_UTF_8_indonesian.o stem_UTF_8_irish.o stem_UTF_8_italian.o stem_UTF_8_lithuanian.o stem_UTF_8_nepali.o stem_UTF_8_norwegian.o stem_UTF_8_porter.o stem_UTF_8_portuguese.o stem_UTF_8_romanian.o stem_UTF_8_russian.o stem_UTF_8_spanish.o stem_UTF_8_swedish.o stem_UTF_8_tamil.o stem_UTF_8_turkish.o utilities.o -L/usr/lib/R/lib -lR
  installing to /home/marcus/R/x86_64-pc-linux-gnu-library/4.1/00LOCK-SnowballC/00new/SnowballC/libs
  ,** R
  ,** inst
  ,** byte-compile and prepare package for lazy loading
  ,** help
  ,*** installing help indices
  ,** building package indices
  ,** testing if installed package can be loaded from temporary location
  ,** checking absolute paths in shared objects and dynamic libraries
  ,** testing if installed package can be loaded from final location
  ,** testing if installed package keeps a record of temporary installation path
  ,* DONE (SnowballC)

  The downloaded source packages are in
          ‘/tmp/RtmpQf0sg2/downloaded_packages’
  #+end_example

  #+begin_src R
    search()
    ls('package:SnowballC')
  #+end_src

  #+RESULTS:
  :  [1] ".GlobalEnv"        "package:SnowballC" "package:tm"        "package:NLP"       "ESSR"             
  :  [6] "package:stats"     "package:graphics"  "package:grDevices" "package:utils"     "package:datasets" 
  : [11] "package:methods"   "Autoloads"         "package:base"
  : [1] "getStemLanguages" "wordStem"

- Which languages are available for stemming?
  #+begin_src R
    getStemLanguages()
  #+end_src

  #+RESULTS:
  :  [1] "arabic"     "basque"     "catalan"    "danish"     "dutch"      "english"    "finnish"    "french"    
  :  [9] "german"     "greek"      "hindi"      "hungarian"  "indonesian" "irish"      "italian"    "lithuanian"
  : [17] "nepali"     "norwegian"  "porter"     "portuguese" "romanian"   "russian"    "spanish"    "swedish"   
  : [25] "tamil"      "turkish"

- Let's check the ~SnowballC::wordStem~ function on an example:
  #+begin_src R
    wordStem(c("learn", "learned", "learning", "learns", "learner"))
  #+end_src

  #+RESULTS:
  : [1] "learn"   "learn"   "learn"   "learn"   "learner"
  : function (words, language = "porter") 
  : NULL

- To apply ~wordStem~ to the cleaned corpus with ~tm_map~, use the
  ~stemDocument~ function, and check another message (25) for success:
  #+begin_src R
    tm_map(sms_corpus_clean,stemDocument) -> sms_corpus_clean
    content(sms_corpus[[25]])  ## before cleaning
    content(sms_corpus_clean[[25]])  ## before cleaning    
  #+end_src

  #+RESULTS:
  : [1] "Could you not read me, my Love ? I answered you"
  : [1] "read love answer"

- Lastly, remove additional whitespace using ~stripWhitespace~, and
  check the first three messages for success using ~lapply~:
  #+begin_src R
    tm_map(sms_corpus_clean,stripWhitespace) -> sms_corpus_clean
    lapply(sms_corpus[1:3],content)
    lapply(sms_corpus_clean[1:3],content)
  #+end_src

  #+RESULTS:
  : $`1`
  : [1] "Hope you are having a good week. Just checking in"
  : 
  : $`2`
  : [1] "K..give back my thanks."
  : 
  : $`3`
  : [1] "Am also doing in cbe only. But have to pay."

* TODO Tokenization - word splitting
  
- The ~DocumenTermMatrix~ function takes a corpus and creates a
  document-term matrix (DTM) with rows as docs and columns as terms:
  #+begin_src R :results silent

  #+end_src
  
- To look at the DTM, transform to a matrix with ~as.matrix~, save the
  matrix as ~m~ and display rows 100 through 105, and columns 100
  through 108.
  #+begin_src R


  #+end_src  

- How sparse exactly is ~m~?
  #+begin_src R


  #+end_src

- In fact, the sparsity is contained in the meta-data of the DTM:
  #+begin_src R

  #+end_src

- You can also create a DTM directly from the raw, unprocessed SMS
  corpus: check the dimensions of the result in the last line and run
  the code block:
  #+begin_src R
    sms_dtm2 <- DocumentTermMatrix(sms_corpus,
                                   control = list(
                                     tolower = TRUE,
                                     removeNumbers = TRUE,
                                     stopwords = TRUE,
                                     removePunctuation = TRUE,
                                     stemming = TRUE))
    
  #+end_src

* TODO Text visualization with ~wordcloud~

- Install and load the ~wordcloud~ package:
  #+begin_src R
    ## install in Org-mode only if options()$repos is set to
    ## cloud.r-project.org/, or set it here like this:
    ## options(repos="https://cloud.r-project.org")
    options()$repos
    # install.packages("wordcloud")


  #+end_src

- Check out the functions in the package:
  #+begin_src R

  #+end_src

- Check out the arguments of the ~wordcloud~ function:
  #+begin_src R

  #+end_src

- A simple example: running the function on a string:
  #+begin_src R :results graphics file 5_everest.png
    string <- "Many years ago the great British explorer George Mallory,
    who was to die on Mount Everest, was asked why did he want to climb it.
    He said, \"Because it is there.\" Well, space is there,
    and we're going to climb it, and the moon and the planets
    are there, and new hopes for knowledge and peace are there.
    And, therefore, as we set sail we ask God's blessing on the
    most hazardous and dangerous and greatest adventure on which
    man has ever embarked."
    
  #+end_src

- Let's do the cleaning explicitly with:
  1) ~qdap::bracketX~ to remove brackets, save in ~stringX~
  2) ~tm::removePunctuation~ to remove punctuation
  3) ~strsplit~ to tokenize
  4) ~unlist~ to transform the ~list~ result to a vector ~tokens~
  #+begin_src R
    ## load qdap and tm packages

    ## clean string with bracketX and save to stringX

    ## remove punctuation from stringX and tokenize

    
  #+end_src

- Run ~wordcloud~ on ~tokens~
  #+begin_src R :results graphics file :file 5_everest1.png

  #+end_src

* TODO Spam vs ham visualization

- A word cloud can be created directly from a ~tm~ corpus:
  1) We use the cleaned corpus ~sms_corpus_clean~
  2) Words must be found in > 1% of the corpus (50/5000)
  3) Place higher-frequency words closer to the center:
  #+begin_src R :results graphics file :file 5_sms_cloud.png

  #+end_src

- See what happens when you change the minimum frequency to 200 and
  10, and the scale (~c(font,cex)~) to different values (~font~ takes
  values 1 to 4, and ~cex~ takes any value. The default is ~c(4,0.5)~.

- Split the data into spam and ham messages using ~subset~:
  #+begin_src R :results silent

    
  #+end_src

- Create two wordclouds side by side looking only at the 30 most
  common words in each of the two sets - can you guess which is which?
  1) set ~max.words~ to 30
  2) set the ~spam~ ~scale~ to ~c(3,0.5)~
  3) set the ~ham~ ~scale~ to ~c(2,0.2)~
  #+begin_src R :results graphics file :file 5_spam_ham_clouds.png
    par(mfrow=c(1,2),pty='m')


  #+end_src

* TODO Creating training and test data

- Get the structure of the document-term-Matrix ~sms_dtm~:
  #+begin_src R

  #+end_src

- Since the SMS messages are already sorted randomly, we simply take
  the first 75% (4,169) messages for training and leave 25% (1,390)
  for testing:
  #+begin_src R :results silent
  
  
  #+end_src

- Check the structure of ~sms_raw~:
  #+begin_src R
  
  #+end_src
 
- Extract the corresponding rows for training and testing labels:
  #+begin_src R :results silent


  #+end_src

- To confirm that the subsets are representative of the complete set
  of SMS data, compute the proportion of spam and ham:
  #+begin_src R

    
  #+end_src

* TODO Reducing training features with ~findFreqTerms~

- What is the dimension of the document-term-matrix ~sms_dtm~?
  #+begin_src R

  #+end_src

- Find the arguments of ~tm::findFreqTerms~:
  #+begin_src R

  #+end_src

- Save the frequent terms of ~sms_dtm_train~ in ~sms_freq_words~, and
  exclude words that appear in less than ~lowfreq=5~ messages:
  #+begin_src R :results silent

  #+end_src

- Check the structure of ~sms_freq_words~:
  #+begin_src R

  #+end_src

- Save the columns ~sms_freq_words~ of in new matrices for training and
  testing:
  #+begin_src R :results silent

    
  #+end_src

* TODO Convert ~numeric~ counts to categorical features

- The conversion function uses ~ifelse~ as a way of testing a condition
  (~x > 0~) for all elements of a vector:
  #+name: convert_counts
  #+begin_src R :results silent


    
  #+end_src

- The ~apply~ function applies its function argument ~FUN~ to all elements
  of an array by row (~MARGIN=1~) or by column (~MARGIN=2~) - we're
  interested in columns:
  #+begin_src R :results silent
    <<convert_counts>>

    
  #+end_src

- The result are our final training and test data in the form of two
  matrices with "No" for 0 and "Yes" for non-zero frequencies:
  #+begin_src R
    ## dimension of sms_train
    ## dimension of sms_test
    ## head of the training data matrix
    ## tail of the test data matrix
  #+end_src

- Taking stock! The ~ls()~ function has a pattern argument. Use it to
  list all objects you've defined so far for the SMS messages: the
  pattern for words starting with "sms" is ~^sms~:
  #+begin_src R

  #+end_src

* TODO Training a classifier on the data

- We use the algorithm implemented in the imaginatively named ~e1071~
  package from the TU Wien[fn:9]. Install and load the package, check that
  it's loaded and take a look at the functions contained in it:
  #+begin_src R
    ## Do this only if options()$repos is set to cloud.r-project.org/





  #+end_src

- Build the model ~sms_classifier~ on the ~sms_train~ dataset with the
  associated ~sms_train_labels~:
  #+begin_src R :results silent

  #+end_src

- The ~sms_classifier~ variable now contains a ~naiveBayes~ classifier
  ~list~ object that can be used to make predictions: let's look at
  1) the class of the model
  2) the data structure of the model
  3) the probabilities for two words from the "spam" and "ham" pile
  #+begin_src R




  #+end_src

* TODO Evaluating model performance

- Apply ~predict~ to the object ~sms_classifier~ with the new data
  ~sms_test~:
  #+begin_src R :results silent

  #+end_src

- Compute the proportional table for the predicted and for the actual
  class labels (~sms_test_pred~ and ~sms_test_labels~):
  #+begin_src R


  #+end_src
  
- How accurate is our classifier? Average over the misidentified
  message labels:
  #+begin_src R

  #+end_src

- For a confidence matrix overview, we use ~gmodels::CrossTable~ with
  reduced cell output (suppressing various proportions):
  #+begin_src R

    
  #+end_src

* TODO Improving model performance

- We build a new classifier with ~laplace=0.1~ adding a small correction
  to the conditional probabilities:
  #+begin_src R :results silent

  #+end_src

- We repeat our prediction with the new classifier:
  #+begin_src R :results silent

  #+end_src

- Check new accuracy:
  #+begin_src R

  #+end_src

- Check new confidence matrix:
  #+begin_src R

  #+end_src


  
  
