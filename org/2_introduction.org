#+TITLE: INTRODUCTION TO MACHINE LEARNING
#+AUTHOR: Marcus Birkenkrahe
#+STARTUP: overview hideblocks indent inlineimages
#+OPTIONS: toc:nil num:nil ^:nil
#+PROPERTY: header-args:R :session *R* :results output :exports both :noweb yes
#+attr_html: float:nil
* README
#+attr_html: :width 600px
[[../img/future_of_ai.png]]

The purpose of today's machine learning (ML) is to assist us with
making sense of the world's massive data collections.

1) Origins, applications and pitfalls of ML

2) How computer use models to transform data into knowledge

3) Steps to match an ML algorithm to your data

4) Extended no-code example and simple R examples

* Origins of ML
#+attr_html: :width 600px
#+caption: Image: Assassin's Creed Origins (2017)
[[../img/1_origins.jpg]]

1) *Statistical modeling* (17th century)[fn:1]

2) *Machine learning* (1950s)[fn:2]

3) *Knowledge discovery* (1980s)[fn:3]

4) *Data mining* (1990s)[fn:4]

5) *Data science* (2010s)[fn:5]

* What about "intelligence"?
#+attr_html: :width 700px
#+caption: ChatGPT's answer to "Are you intelligent?" (2023)
[[../img/1_intelligence.png]]

#+attr_html: :width 700px
#+caption: ChatGPT's answer to "Are you intelligent?" (2025)
[[../img/intelligence.png]]

* Uses and abuses of ML
#+attr_html: :width 600px
[[../img/1_deepblue.png]]

- 1997: Deep Blue defeats Kasparov; 2011: Watson wins Jeopardy.

- Machines are pure horsepower without direction - they need a human
  to motivate the analysis and turn the result into meaningful
  action - like a hound and a human.

- Interesting analysis of Kasparov vs. Deep Blue ([[https://theconversation.com/twenty-years-on-from-deep-blue-vs-kasparov-how-a-chess-match-started-the-big-data-revolution-76882][Anderson, 2017]])
  + A coding bug may have misled Kasparov to overestimate Deep Blue
  + Conspiracy? Deep Blue may have have been a "[[https://en.wikipedia.org/wiki/Mechanical_Turk][Mechanical Turk]]"
  + Illustrates the difference between man and machine
  + Kasparov seems to have had a mental breakdown

* ML successes
#+attr_html: :width 400px
#+caption: Inside of a Tesla car driving autonomously
[[../img/1_tesla.jpg]]

Many different uses, many different models:
- *Identification* of unwanted spam messages in email
- *Segmentation* of customer behavior for targeted advertising
- *Forecasts* of weather behavior and long-term climate changes
- *Prevention* of fraudulent credit card transactions
- *Estimation* of actuarial financial damage of natural disasters
- *Prediction* of popular election outcomes
- *Autonomous* vehicles: auto-piloting drones and self-driving cars
- *Optimization* of energy use in homes and office buildings
- *Projection* of areas where criminal activity is most likely
- *Discovery* of genetic sequences linked to diseases

* Limits of ML

- Little flexibility outside of strict parameters and no common sense.

- Consequences of releasing a ML model are hard to predict[fn:6].

- Inability to make simple inferences about logical next steps
  (e.g. repeatedly served banners on ecommerce sites).

- Random epic failures: handwriting recognition, 1994
  #+caption: Lisa on Ice, The Simpsons, 20th Century Fox (1994)
  #+attr_html: :width 400px
  [[../img/lantz_2.jpg]]

- Auto-correct failures (ML insists on what you once wanted or were).

- Natural language processing (audio/text mining) is still very difficult.

- Alas, we often adapt to the limited abilities of our machines.

* ML ethics
#+caption: Boston Dynamics - US Marines Robot Service Dog
#+attr_html: :width 500px
[[../img/1_ethics.jpg]]

- Like any *tool*, it can be used for "good" or for "evil"
- Associated *legal* issues and social norms are still uncertain
- Issues include *privacy* rights of customers
- Handing *critical* operations (e.g. airport control) to machines
- Relying on ML in *life-or-death* situations (medical diagnosis)
- *Blindly* applying ML analysis results to make decisions
- *Perpetuating* discrimination based on race or gender
- *Reinforcing* negative stereotypes
- *Anonymizing* data is difficult because ML is good at finding you
- *Regulation*, e.g. EU's General Data Protection Regulation (GDPR)
- ML can be used for fake news, or *misguiding* autonomous systems

* Extended example: supervised learning
#+attr_html: :width 400px
#+caption: Poster, Labor Management Committee, 1950s
[[../img/1_supervisor.jpg]]

*Process:*
1) Build a *classification* model from *known* data instances
2) Test model to *classify* newly presented *unknown* data instances
3) Translate model into algorithmic *production* rules

* Building a model from training data

- Dataset: hypothetical training data for a disease diagnosis
  #+attr_html: :width 600px
  #+caption: Hypothetical disease diagnosis training data (Source: Roiger, 2021)
  [[../img/1_patientdata.png]]

- Observation: Patient 1 has a sore throat, fever, swollen glands, is
  congested and has a headache. He was diagnosed with strep throat.

- A /decision tree/ can be used to generalize a set of input instances
  as shown and transform it into rules.

- To generalize, we must make assumptions about the relative
  importance of attributes and their relationship.

- For example:
  + If a patient has swollen glands, the diagnosis is strep throat
  + If a patient does not have swollen glands and a fever, it's a cold
  + If a patient does not have swollen glands nor a fever, it's allergy
  #+attr_html: :width 400px
  #+caption: Decision tree for disease diagnosis (Source: Roiger, 2021)
  [[../img/1_decision_tree.png]]

- Notice that the attributes /sore throat/, /congestion/ and /headache/ do
  not enter our diagnostic prediction.

* Testing the model on unknown instances
#+attr_html: :width 400px
#+caption: Decision tree for disease diagnosis
[[../img/1_decision_tree.png]]

- Moving on to a new data set with unknown classification, i.e. no
  diagnosis

- Use the decision tree to classify the first two instances:
  #+attr_html: :width 600px
  #+caption: Test data for disease diagnosis model (source: Roiger, 2021)
  [[../img/1_testing.png]]

- Patient 11 has swollen glands but no fever => strep throat

- Patient 12 has no swollen glands but fever => cold

* Translate model into production rules
#+attr_html: :width 400px
#+caption: Decision tree for disease diagnosis (source: Roiger, 2021)
[[../img/1_decision_tree.png]]

- General form of a /production rule/ looks like pseudocode[fn:7]:
  #+begin_example
    IF antecedent condition
    THEN consequent conditions
  #+end_example
- The three /production rules/ for the decision tree:
  #+begin_example
    IF swollen glands = YES
    THEN diagnosis = strep throat

    IF swollen glands = No & Fever = Yes
    THEN diagnosis = cold

    IF swollen glands = No & Fever = No
    THEN diagnosis = allergy
  #+end_example
- Testing the rules on patient 13 yields: diagnosis = allergy
  #+attr_html: :width 600px
  #+caption: Test data for disease diagnosis model (source: Roiger, 2021)
  [[../img/1_testing.png]]

* DONE Practice: Testing and Improving a Classification Model

In this class exercise, you learn to classify, model, and update a
production model by hand using a table, pseudocode, and a diagram.

Download the practice file from [[https://tinyurl.com/ml-intro-practice-org][tinyurl.com/ml-intro-practice-org]]

* How machines learn

- Unlike humans, machines need explicit conditions and instructions
  literally down to the letter (ML does not change that completely).

- What's the effect for humans when making everything very explicit?
  *Does explicitness help or hinder human learning?*[fn:8]

- To be a strong data scientist / ML practitioner requires solid
  understanding of *how the learning algorithms work*.

- Example: The =lm= function in R for linear regression abstracts away
  a lot of matrix algebra to solve a system of linear equations.

- Basic ML process:
  #+attr_html: :width 700px
  #+caption: General machine learning process (Source: Lantz, 2019)
  [[../img/1_lantz_3.jpg]]

- How does the extended diagnosis example fit in this scheme?[fn:9]

* Data storage = observe + memorize + recall
#+attr_html: :width 400px
[[../img/1_sheldon.png]]

- *Data storage* utilizes observation, memory, and recall to provide a
  factual basis for further reasoning

- Storage needs to take software and hardware conditions into
  account - how?[fn:10]

- You need to store raw data selectively - *more data* does not
  necessarily mean *more information* (too much data can obscure what
  you're looking for) and carries a performance overhead

- Remember studying for an exam - do you gorge yourself on all
  available details or do you select questions and answers that were
  discussed in class?[fn:11]

- See Kupek (2023): "[[https://stackoverflow.blog/2023/01/04/getting-your-data-in-shape-for-machine-learning/][Getting your data in shape for machine learning]]"
  with a (Python) Google Colaboratory [[https://colab.research.google.com/drive/1mg2WRO7_DIc1U_0Q1NO7ou6C4F89NuWY?usp=sharing][notebook for coding examples]].

* DONE Nile example: data storage

- To run the code below, open [[https://tinyurl.com/2mnv425w][tinyurl.com/2mnv425w]], save the
  file to ~1_ml_intro_practice.org~ and open it in Emacs.

- Example: the following numbers come from R's ~Nile~ data set:
  #+begin_example
  1120 1160 963 1210 1160 1160 813 1230 1370 1140
  995 935 1110 994 1020 960 1180 799 958 1140
  1100 1210 1150 1250 1260 1220 1030 1100 774 840
  874 694 940 833 701 916 692 1020 1050 969
  831 726 456 824 702 1120 1100 832 764 821
  768 845 864 862 698 845 744 796 1040 759
  781 865 845 944 984 897 822 1010 771 676
  649 846 812 742 801 1040 860 874 848 890
  744 749 838 1050 918 986 797 923 975 815
  1020 906 901 1170 912 746 919 718 714 740
  #+end_example

- To extract the data from the data set (already stored in R):
  #+begin_src R :results silent
    write(x=Nile,
          file="../data/Nile.txt", # Unix-style forward slash
          ncolumns=1,
          sep=" ")
  #+end_src

- The values are stored as a text file ~Nile.txt~ of size 440 byte,
  which means 440 * 8 = 3520 bits, or binary value capacitors:
  #+begin_src R
    ##shell(cmd="DIR ..\\data\\Nile.txt") # escaped Windows backward slash
    system("file ../data/Nile.txt")
  #+end_src

  #+RESULTS:
  : ../data/Nile.txt: ASCII text

- When on disk, ~Nile.txt~ is stored in non-volatile memory (it's
  permanent). When it is loaded into R (or another shell program), it
  is represented as RAM (Random Access Memory), physically realized as
  a capacitor that is charged (1) or uncharged (0) ([[http://androidgrl.github.io/2019/01/01/binary/][source]]).
  #+attr_html: :width 400px
  [[../img/1_lantz_dramcapacitor.png]]

- You can look at the text file using ~notepad~:
  #+begin_src R :results silent
    ##shell(cmd="notepad ..\\data\\Nile.txt")
    system("cat ../data/Nile.txt")
  #+end_src

* Abstraction = transform + train
#+attr_html: :width 400px
#+caption: Margritte, La Trahison Des Images, lacma.org
[[../img/1_lantz_4.jpg]]

- *Abstraction* involves translating stored data into broader
  representations and concepts

- Abstraction needs to take available computing data structures into
  account - how?

- The nature of a "representation" is that it is *not the
  original* [fn:12]

- For ML, recognition is more important than reality: the "AI" is not
  trying to build a world, but to translate it into something it can
  "see" so that it can process it further [fn:13]

* DONE Nile example: transformation

- ~Nile~ example: earlier, we stored integer numbers in memory. A
  convenient representation in R involves choosing a *data structure*
  and transforming the numbers into it

- We read the text data from file using the R function ~read.table~ and
  store them in a time series using the R function ~ts~:
  1) read the text file ~read.table~ as a ~data.frame~
  2) remove column name with ~colnames~
  3) create time series with ~ts~ from data frame
  #+begin_src R :results output
    nile_df <- read.table(
      file="../data/Nile.txt",   # read from text file
      sep="",                  # entries separated by empty space
      header=FALSE)             # no 1st row with attribute information
    colnames(nile_df) <- NULL
    ##nile_df
    nile_ts <- ts(nile_df,start=1871) # convert data into time-series
    nile_ts
  #+end_src

  #+RESULTS:
  #+begin_example
  Time Series:
  Start = 1871 
  End = 1970 
  Frequency = 1 
         [,1]
    [1,] 1120
    [2,] 1160
    [3,]  963
    [4,] 1210
    [5,] 1160
    [6,] 1160
    [7,]  813
    [8,] 1230
    [9,] 1370
   [10,] 1140
   [11,]  995
   [12,]  935
   [13,] 1110
   [14,]  994
   [15,] 1020
   [16,]  960
   [17,] 1180
   [18,]  799
   [19,]  958
   [20,] 1140
   [21,] 1100
   [22,] 1210
   [23,] 1150
   [24,] 1250
   [25,] 1260
   [26,] 1220
   [27,] 1030
   [28,] 1100
   [29,]  774
   [30,]  840
   [31,]  874
   [32,]  694
   [33,]  940
   [34,]  833
   [35,]  701
   [36,]  916
   [37,]  692
   [38,] 1020
   [39,] 1050
   [40,]  969
   [41,]  831
   [42,]  726
   [43,]  456
   [44,]  824
   [45,]  702
   [46,] 1120
   [47,] 1100
   [48,]  832
   [49,]  764
   [50,]  821
   [51,]  768
   [52,]  845
   [53,]  864
   [54,]  862
   [55,]  698
   [56,]  845
   [57,]  744
   [58,]  796
   [59,] 1040
   [60,]  759
   [61,]  781
   [62,]  865
   [63,]  845
   [64,]  944
   [65,]  984
   [66,]  897
   [67,]  822
   [68,] 1010
   [69,]  771
   [70,]  676
   [71,]  649
   [72,]  846
   [73,]  812
   [74,]  742
   [75,]  801
   [76,] 1040
   [77,]  860
   [78,]  874
   [79,]  848
   [80,]  890
   [81,]  744
   [82,]  749
   [83,]  838
   [84,] 1050
   [85,]  918
   [86,]  986
   [87,]  797
   [88,]  923
   [89,]  975
   [90,]  815
   [91,] 1020
   [92,]  906
   [93,]  901
   [94,] 1170
   [95,]  912
   [96,]  746
   [97,]  919
   [98,]  718
   [99,]  714
  [100,]  740
  #+end_example

- The transformed data set contains additional information that was
  not present in the numbers themselves. We have used additional
  information (about the origin of the data) and R's time series data
  structure.
  #+begin_src R
    str(nile_ts)
    class(nile_ts)
    str(Nile)
  #+end_src

  #+RESULTS:
  :  Time-Series [1:100, 1] from 1871 to 1970: 1120 1160 963 1210 1160 1160 813 1230 1370 1140 ...
  :  - attr(*, "dimnames")=List of 2
  :   ..$ : NULL
  :   ..$ : NULL
  : [1] "ts"
  :  Time-Series [1:100] from 1871 to 1970: 1120 1160 963 1210 1160 1160 813 1230 1370 1140 ...

* Modeling
#+attr_html: :width 600px
#+caption: Source: Hosseini, Hytönen, Kinnunen (2022)
[[../img/1_lantz_gestalt.png]]

- When a machine creates a *Knowledge representation*, it summarizes
  stored raw data using a *model*, an explicit description of the
  patterns within the data

- A model represents an idea greater than the sum of its parts (also:
  "The whole is greater than the sum of its parts")[fn:14]

- Machines, unlike humans, cannot comprehend these Gestalt patterns as
  a whole, they can only sequentially process the components of a
  pattern[fn:15].

- There are many different types of models, including:
  + Mathematical equations
  + Relational diagrams, such as trees and graphs
  + Logical if/else rules (conditional structures)
  + Groupings of data (clusters)

- Typically, the machine does not pick the model - it is picked by a
  human depending on the learning task and the type of data available

* DONE Nile example - modeling

- As an example of statistical inference, we use the time series data
  of ~Nile~ to create a statistical model

- In R this is easily achieved with the ~summary~ function
  #+begin_src R
    data(Nile)  # add the built-in Nile dataset to the session
    ls()  # show all R objects in the current session
    summary(Nile) # 5-point summary + sample average
  #+end_src

  #+RESULTS:
  : [1] "Nile"    "nile_df" "nile_ts"
  :    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  :   456.0   798.5   893.5   919.4  1032.5  1370.0

- To visualize this model, you can use ~boxplot~ (and ~abline~ to add the
  ~mean~):
  #+begin_src R :results graphics file :file boxplot.png
    boxplot(Nile,  # target data
            las=1, # reorient x-axis labels
            horizontal=TRUE, # show boxplot horizontally
            main="Annual flow of the Nile at Aswan\nbetween 1871 and 1970",
            xlab="Nile volume (mio cubic meters)")
    abline(v=mean(Nile),  # draw a vertical line
           col="blue",    # paint line blue
           lwd=2)         # double line width
  #+end_src

  #+RESULTS:
  [[file:boxplot.png]]

- The /generic/ function ~summary~ collapses the abstraction (time series
  representation) into a statistical summary

- The data pattern (distribution) is lost here:
  #+begin_src R :file plot.png :results output graphics file :exports both :comments both :tangle yes :noweb yes
    plot(Nile)  # automatically picks a line plot
  #+end_src

  #+RESULTS:
  [[file:plot.png]]

- What about the dip? When was it?
  #+begin_src R
    time(Nile)[Nile==min(Nile)]
  #+end_src

  #+RESULTS:
  : [1] 1913

- That ~summary~ is /generic/ is relevant because it means that it can
  deal with many different abstractions (and models, too):
  #+begin_src R
    methods(summary)
  #+end_src

  #+RESULTS:
  #+begin_example
   [1] summary.aov                    summary.aovlist*               summary.aspell*               
   [4] summary.check_packages_in_dir* summary.connection             summary.data.frame            
   [7] summary.Date                   summary.default                summary.ecdf*                 
  [10] summary.factor                 summary.glm                    summary.infl*                 
  [13] summary.lm                     summary.loess*                 summary.manova                
  [16] summary.matrix                 summary.mlm*                   summary.nls*                  
  [19] summary.packageStatus*         summary.POSIXct                summary.POSIXlt               
  [22] summary.ppr*                   summary.prcomp*                summary.princomp*             
  [25] summary.proc_time              summary.srcfile                summary.srcref                
  [28] summary.stepfun                summary.stl*                   summary.table                 
  [31] summary.tukeysmooth*           summary.warnings              
  see '?methods' for accessing help and source code
  #+end_example

* ML training
#+attr_html: :width 400px
#+caption: Runners, photo from Unsplash
[[../img/1_train.png]]

- Machine learning models are trained. This means that the model is
  fitted to a data set

- Once the model is trained, it has been transformed into an abstract
  form ("black box") that summarizes (and transcends) the original
  information

- The training model is not "learning" yet because the result still
  must be evaluated (tested) before the model is ready.

* Training a physics model

- Example from physics: by fitting equations to observational data,
  Newton inferred the concept of gravity (we think). It was always
  present but not recognized:
  #+attr_html: :width 600px
  #+caption: Modeling example in physics (Lantz, 2019)
  [[../img/1_lantz_5.png]]

- In R: ~g~ is the acceleration due to gravity[fn:16]
  #+begin_src R
    x <- c(4.9,19.6,44.1,78.5) # distance observations
    t <-c(1,2,3,4)  # time observations
    2*x/(t^2) # fit data to equation and compute g
    as.numeric(format(2*x/(t^2),digits=2)) # compute, print 2 digits
  #+end_src

  #+RESULTS:
  : [1] 9.8000 9.8000 9.8000 9.8125
  : [1] 9.8 9.8 9.8 9.8
  : [1] 7

- Code question: Why does =2*d/(t^2)= work here given that both =d= and =t=
  are vectors? Why don't I have to use a =for= loop to compute this?
  #+begin_quote
  R has /vectorization/, which means that permitted vector operations
  are executed element-wise without further instruction.
  #+end_quote

- How would you compute this with a =for= loop?
  #+begin_src R
    for (i in t)
      print(x[i] * 2 / t[i]^2)
  #+end_src

  #+RESULTS:
  : [1] 9.8
  : [1] 9.8
  : [1] 9.8
  : [1] 9.8125

- Alternative: =sapply= with an anonymous ('lambda') function:
  #+begin_src R
    g = sapply(X=x, FUN=function(x){2*x/t^2})
    as.numeric(format(diag(g),digits=2))
  #+end_src

  #+RESULTS:
  : [1] 9.8 9.8 9.8 9.8

- Puzzle: This other table of distance + time data was found on a
  napkin that Elon Musk had scribbled on and left behind at
  Mar-a-Lago, Florida - what's going on?

  | Distance [m] | Time [s] |
  |--------------+----------|
  |         1.86 |        1 |
  |         7.42 |        2 |
  |        16.70 |        3 |
  |        29.68 |        4 |

- The gravitational constant works out to:
  #+begin_src R
    x2 <- c(1.86,7.42,16.7,29.68)
    t2 <- 1:4
    g2 = sapply(X=x2, FUN=function(x){2*x2/t2^2})
    as.numeric(format(diag(g2),digit=2))
  #+end_src

  #+RESULTS:
  : [1] 3.7 3.7 3.7 3.7

- What's the explanation?
  #+begin_quote
  "The gravitational constant of 3.7   m/s 2 3.7m/s 2 in the data
  aligns with the gravity on Mars. The napkin likely reflects
  calculations for motion under Martian conditions, fitting Elon
  Musk's interests in space exploration and Mars colonization."
  #+end_quote

- Other model examples include:
  1. Genomic data models identify genes responsible for disease
  2. Bank transaction models identify fraudulent activities
  3. Psychological models identify new disorders
  4. Medical models identify diagnostic patterns

- These patterns were always there but had not been identified/seen
  prior to presenting the information in a different format.[fn:17]

* Nile example - training a density model

- The ~truehist~ function fits the dataset to a density estimate, and
  ~density~ does the same with a smoothing effect added:
  #+begin_src R :results graphics file :file ../img/lantz_nile.png
    library(MASS)     # load MASS package
    truehist(Nile,    # target dataset for histogram
             las=1,   # reorient axis labels
             xlab="", # remove default x-axis annotation
             main="") # remove default title
    par(new=TRUE)     # allow plotting over previous plot
    plot(density(Nile), # target dataset for plot
         col="red",   # draw line in red
         col.lab="red", # color axis label red
         lwd=2,       # double line width
         xaxt="n",    # suppress plotting x-axis
         yaxt="n",    # suppress plotting y-axis
         main="")     # remove title
    title("Flow through the Nile 1872-1970")
  #+end_src

  #+RESULTS:
  [[file:../img/lantz_nile.png]]

* Nile example: training a linear model

-  The ~lm~ function needs points to fit a line through. ~Nile~ only has
  two vectors, one is the ~Nile~ values, the other one is the ~time~ of
  each instance of ~Nile~. Apply the function ~time~ to ~Nile~:
  #+begin_src R
    time(Nile)
  #+end_src

- The ~lm~ function attempts to fit a linear model to the ~Nile~ dataset:
  #+begin_src R :results graphics file :file ../img/lantz_nile_lm.png
    ## create the linear model (needs 2 dimensions)
    model <- lm(Nile ~ time(Nile))

    ## plot Nile data
    plot(Nile,
         type="p",  # plot points only
         col="blue", # plot points in blue
         pch=16,  # point character solid circle
         ylab="Flow in mio cubic metres") # y-axis annotation

    ## draw the model - a trendline
    abline(model,  # model consists of intercept and slope
           col="red", # red line
           lwd=2)   # double line width

    ## connect Nile data by black dashed lines
    lines(Nile,
          type="l",
          col="black",
          lty=2)

    ## title plot
    title("Flow through the Nile at Assuan 1872-1970")

    ## add a legend
    legend("topright",  # where the legend is located
           legend=c("Observation", "Linear Model"),
           pch = c(16,NA),  # assign point character
           lty = c(NA, 2),  # assign line type
           col = c("blue", "red")) # assign color
  #+end_src

- This last example demonstrates "underfitting" = most points are not
  well represented by the model. However, the general trend is well
  represented by the red line: over time, the water flow through the
  Nile at Assuan decreased.

* Generalization

- *Generalization* uses abstracted data to create knowledge and
  inferences that drive action in new contexts

- To do this, the machine searches through an entire set of models
  (equivalent to theories of prediction or inference) employing a
  process called "heuristics" (finding skills or educated guesses)

- Compare it to a Google search that you perform yourself: in response
  to the output of the search you refine your search string, e.g.
  1) "generalization" (in response to the too general result)
  2) "generalization reasoning" (in response to Google's completion)
  3) "generalization reasoning models" (in response to your interest)
  4) "generalization models" (in response to the too specific result)
  5) "generalization machine learning" (result still too
     specific)[fn:20]

- *Human heuristics* are guided by emotion and can be fallible -
  e.g. "availability heuristics", the tendency to estimate likelihood
  of an event depending on how easily examples can be recalled
  (e.g. airline accidents over vehicle accidents)

- Misapplied *machine heuristics* as a result of algorithmic errors are
  called *bias* if the conclusions are /systematically/ erroneous
  (i.e. wrong in a consistent or predictable manner)

- Example: an ML algorithm that generalizes faces to have two circles
  above a mouth would not identify a face with glasses.
  #+attr_html: :width 500px
  #+caption: Machines cannot do heuristics (Lantz, 2019)
  [[../img/1_lantz_6.png]]

- [ ] Could "a little bias" also be useful?[fn:18]

* Evaluation + overfitting
#+attr_html: :width 400px
#+caption: The One Ring To Rule Them All
[[../img/1_ring.png]]
#+begin_quote
"There is no single learning algorithm to rule them all." -Brett Lantz
#+end_quote

- No ML approach is best for every problem - an application of the
  rigorous "No Free Lunch" (NFL) theorem for search and optimization
  ([[https://fab.cba.mit.edu/classes/865.18/design/optimization/nfl.pdf][Wolpert/Macready, 2005]])

- *Evaluation* provides a feedback mechanism to measure the utility of
  learned knowledge and inform potential improvements

- After training on an initial training dataset, the model is
  evaluated on a separate test dataset of new, unseen cases

- Models fail to generalize perfectly due to noise, unexplained or
  inexplicable variations in data due to
  1) *measurement* errors (e.g. imprecise sensors)
  2) *human* subject issues (e.g. random answers in surveys)
  3) data *quality* issues (missing, null, truncated, corrupted values)
  4) *complex* phenomena whose impact appears to be random

- Famous noise that turned into gold: [[https://www.esa.int/Science_Exploration/Space_Science/Herschel/Cosmic_Microwave_Background_CMB_radiation][cosmic microwave background]]
  radiation that is attributed to an echo of the 'Big Bang'

- Modeling noise is called /overfitting/.[fn:19]
  #+attr_html: :width 500px
  #+caption: Overfitting (Lantz, 2019)
  [[../img/1_lantz_overfitting.jpg]]

* TODO Assignment: Fitting a model to obtain g

Create a linear model using R's =lm= function to predict the falling
=distance= at =time= = 5 seconds.

Compare the prediction with the value for the =distance= at that =time=
computed with the formula for =distance= as a function of =time=.

Upload your notebook solution (or a URL to an online notebook) to
Canvas.

* Summary

- ML can find actionable insight in large data sets

- ML involves *abstraction* of data into structured *representation* and
  *generalization* of the structure into action that can be *evaluated*

- Data that contains examples/observations/records and features of the
  concept to be learnt is summarized in a *model*

* ML Glossary

| TERM             | MEANING                                |
|------------------+----------------------------------------|
| Machine learning | Computer solves task with models       |
| Data abstraction | Transform raw data to table structure  |
| Generalization   | Trained model fits unknown data        |
| Evaluation       | Model feedback to test accuracy        |
| Heuristics       | Model to find solution quickly         |
| Bias             | Machine heuristics that lead to errors |
| Overfitting      | Modeling noise instead of signals      |
| Underfitting     | Model is too simple for the data       |

* TODO Practice/Assignment: ML Code Glossary

- Create a table in Org-mode by typing ~|~ for each new column

- Collect all (20) functions from the practice file and explain them

- To highlight code, you can surround it with a ~~~ character

- To "fix" the table alignment, use ~M-q~ (ALT + q)

| NO | FUNCTION | MEANING |
|----+----------+---------|
|    |          |         |

* References

- Anderson (2017). Twenty years on from Deep Blue vs Kasparov: how a
  chess match started the big data revolution. [[https://theconversation.com/twenty-years-on-from-deep-blue-vs-kasparov-how-a-chess-match-started-the-big-data-revolution-76882][@theconversation.com.]]

- Hosseini, Z., Hytönen, K., & Kinnunen, J. (2022). Improving Online
  Content Quality Through Technological Pedagogical Content Design
  (TPCD). In S. Vachkova, & S. S. Chiang (Eds.), Education and City:
  Quality Education for Modern Cities, vol 3. European Proceedings of
  Educational Sciences (pp. 284-296). European
  Publisher. https://doi.org/10.15405/epes.22043.25

- Kupek (Jan 4, 2023). Getting your data in shape for machine
  learning. URL: [[https://stackoverflow.blog/2023/01/04/getting-your-data-in-shape-for-machine-learning/][stackoverflow.blog]].

- Lantz (2019). Machine Learning with R. Packt.

- Roiger (2020). Just Enough R!. CRC Press.

- Serrano (2021). Grokking Machine Learning.

* Footnotes

[fn:20]The Google ML crash course has a nice video on "generalization"
in ML [[https://developers.google.com/machine-learning/crash-course/overfitting/generalization][here]]. This [[https://developers.google.com/machine-learning][whole no-code course]] seems quite good albeit a little
on the naive side.

[fn:1] *Statistical* methods make assumptions about the nature of the
data. If these assumptions are violated the models built with the data
are inaccurate. Systematically developed in the 17th and 18th century,
popular since the rise of computers in the mid-20th century.

[fn:2] For *Machine Learning*, assumptions about data distributions and
variable independence are not a concern. ML is part of the field of
Artificial Intelligence (AI) - computers who make their own
decisions. Popular since 1959 (Arthur Samuel: "ML is the ability to
learn without being explicitly programmed."). Subfields include
deep learning = ML with artificial neural networks; reinforcement
learning = reward-based machine learning

[fn:3] *Knowledge discovery* in databases (KDD) was coined in 1989 to
emphasise that knowledge can be derived from data-driven
discovery. Frequently used interchangeably with data
mining. Includes not just the pattern search but also methods for
extracting, preparing and using data.

[fn:4] *Data mining* is the process of using one or more machine
learning algorithms to find patterns or structure in data. The
patterns may be: a set of rules, a graph or network, a tree, one or
more equations, etc. The applications can be part of a visual
dashboard or as simple as a list. Popular since ca. 1995.

[fn:5] *Data science* refers to the process of extracting meaningful
knowledge from data, using methods from statistics, computer
science, database management and more. ML is not required for data
science but it is often used - e.g. when fitting a trendline to a
dataset. Popular since ca. 2012. Subfields include data
engineering, process mining, and machine learning.

[fn:6] See Loizos (Dec 9, 2022): "[[https://techcrunch.com/2022/12/09/is-chatgpt-a-virus-that-has-been-released-into-the-wild/][Is ChatGPT a 'virus that has been
released into the wild'?]]" - 2019 interview with Sam Altman (OpenAI)

[fn:7]This is exactly what pseudocode is: natural language without the
constraints of syntactical rules. What used to be helpful in the past
could in the future well become the standard for programming, cp.

[fn:8]How did you "learn" to close a door? Did your mom give you a
long lecture about the physics, the difference in temperature on
either side of the door, and about the different ways to grip and
turn the handle? What happens when you encounter a different handle
you have not seen yet? What if you encounter a door that has no
handle?

[fn:9]Diagnosis ML example: (1) Data storage: raw patient data; (2)
Abstraction: table of attributes and records; (3) Generalization:
rules from known diagnosis; (4) Evaluation: prediction of unknown
diagnosis

[fn:10] Storage needs to take software and hardware conditions into
account: (1) performance: access speed; (2) data organisation,
e.g. relational or non-relational data; (3) missing or otherwise
contaminated data; (4) R: all data is in memory (space issue); (5)
SQLite: one file non-concurrent access (security/usability issues)

[fn:11]In fact, human learning is poorly understood: if you have an
eidetic memory ([[https://youtu.be/A4ugfCjqlZ4][Sheldon-Cooper-style]]), storing everything may be a
valid strategy. I don't think I have that but I still like to fill
myself up with seemingly "irrelevant" data - and I trust my guardian
angel, or my intuition, or whatever you will, to pull the proverbial
rabbit out of a hat when needed. This has often worked for me!

[fn:12]René Magritte's painting "The Treachery of Images" illustrates
the idea of a representation: /"Ceci n'est pas une pipe"/ because it's
an image of a pipe, and not the pipe itself.

[fn:13]Contrary to humans, for whom "seeing is believing", which
implies that we cannot see without processing.

[fn:14]In visual perception, the idea of a model summarizing data is
illustrated by the six Gestalt (German for "shape") principles: each
of them implies not just the pixels of the image but a pattern that
leads a human perceiving more than just the pattern itself.

[fn:15]This was one of the critiques of AI by philosopher Hubert
Dreyfus ([[https://en.wikipedia.org/wiki/Hubert_Dreyfus#Dreyfus'_criticism_of_AI][see Wikipedia here]] and here for a [[https://debategraph.org/Details.aspx?nid=2785][graph representation]]).

[fn:16]For the back story on this, I asked a fully trained model,
ChatGPT: "How far does an object of 1 kg fall in 1 second?" - very
complete answer, check it out: [[https://tinyurl.com/mr2sm6zx][tinyurl.com/mr2sm6zx]].

[fn:17]There is also the danger here - all predictions are stochastic
in nature, i.e. they are probabilistic predictions, likelihoods,
only. And the testing as well as the evaluation is rife with
assumptions. One may ask: how permanent are the results, which are
unlike gravity, subject to cultural definitions ("fraud", "disorder",
"disease") hence not as objective as physical, observable laws?

[fn:18]Bias (like presets) allows us to favor some choices over others
and discard some choices as irrelevant (or immoral/toxic). The net
effect could be that we become more action-oriented and less bogged
down by search. Bias is also how ML algorithms choose among many ways
to understand data.

[fn:19]Compare this with the very similar looking diagram earlier, the
linear trendline modeling of the ~Nile~ time series data. Underfitting
misses out on available information, while overfitting interprets
noise (irrelevant information) as meaningful for the pattern.
